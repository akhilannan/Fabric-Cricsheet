{"cells":[{"cell_type":"markdown","source":["# Install Semantic Link"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"084085d1-6ce8-421e-a876-b7bffd3d084d"},{"cell_type":"code","source":["!pip install --upgrade semantic-link --q"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"87d45121-7c77-4174-8566-d631516d0849"},{"cell_type":"markdown","source":["# Import Libraries"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"078e2c43-91ae-4e3d-81cb-70be3d00873b"},{"cell_type":"code","source":["from pyspark.sql import functions as F\n","from pyspark.sql import types as T\n","from pyspark.sql import DataFrame\n","from pyspark.sql import SparkSession\n","from delta.tables import DeltaTable\n","from urllib.request import urlretrieve\n","from notebookutils import mssparkutils\n","from sempy import fabric\n","from sempy.fabric.exceptions import FabricHTTPException\n","from concurrent.futures import ProcessPoolExecutor\n","import pandas as pd\n","import pathlib\n","from zipfile import ZipFile\n","import os\n","import datetime\n","import dateutil\n","import pytz\n","import time\n","import uuid\n","import ast\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","import requests\n","import json\n","import base64\n","import math"],"outputs":[],"execution_count":null,"metadata":{},"id":"4c1d5f31-6e15-42e4-82a9-1809fe7b4265"},{"cell_type":"markdown","source":["# Initialize Spark Session"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"349f0c11-9a85-41e1-9916-9a3a702c1c6f"},{"cell_type":"code","source":["spark = SparkSession.builder.getOrCreate()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"2268eefa-0bcc-4700-9484-0fd1ae6c7f13"},{"cell_type":"markdown","source":["# Get Fabric Items"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"2f881736-9f17-4879-8dd1-88d7f2ad831a"},{"cell_type":"code","source":["def get_fabric_items(item_name=None, item_type=None, workspace_id=fabric.get_workspace_id()):\n","    \"\"\"\n","    Retrieve fabric items based on optional filters for name and type within a workspace.\n","\n","    Args:\n","        item_name: Optional; the display name of the item to filter by.\n","        item_type: Optional; the type of the item to filter by.\n","        workspace_id: Optional; the workspace ID to search within. Defaults to the current workspace.\n","    \n","    Returns: A pandas dataframe of fabric items matching the criteria.\n","    \"\"\"\n","    # Build the query conditions\n","    conditions = []\n","    if item_name:\n","        conditions.append(f\"`Display Name` == '{item_name}'\")\n","    if item_type:\n","        conditions.append(f\"`Type` == '{item_type}'\")\n","\n","    # Construct the query string\n","    query = \" and \".join(conditions)\n","\n","    # Perform the query if conditions exist, otherwise list all items\n","    items = fabric.list_items(workspace=workspace_id)\n","    return items.query(query) if conditions else items\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"cac09a3f-e78b-44bb-b4d5-17a3621354d7"},{"cell_type":"markdown","source":["# Get Lakehouse ID"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"d0d8741f-c23c-4a9b-9c7d-078b41cb032b"},{"cell_type":"code","source":["def get_lakehouse_id(lakehouse_name: str) -> int:\n","    \"\"\"Returns the Id of a Lakehouse item given its display name.\n","\n","    Args:\n","        lakehouse_name: The display name of the Lakehouse item.\n","\n","    Returns:\n","        The Id of the Lakehouse item as an integer.\n","\n","    Raises:\n","        ValueError: If no Lakehouse item with the given display name is found.\n","    \"\"\"\n","\n","    # Filter the list by matching the display name with the given argument\n","    query_result = get_fabric_items(item_name= lakehouse_name, item_type = 'Lakehouse')\n","\n","    # Check if the query result is empty\n","    if query_result.empty:\n","        # If yes, raise an exception with an informative message\n","        raise ValueError(f\"No Lakehouse item with display name '{lakehouse_name}' found.\")\n","        \n","    # If no, return the first value of the Id column as an integer\n","    return query_result.Id.values[0]"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"f9fa4d22-c11b-49f7-9819-f41999a02e0b"},{"cell_type":"markdown","source":["# Create Lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"f908818c-8b6e-4f56-8c27-c7047a825f55"},{"cell_type":"code","source":["def create_lakehouse_if_not_exists(lh_name: str) -> str:\n","    # Use a docstring to explain the function's purpose and parameters\n","    \"\"\"Creates a lakehouse with the given name if it does not exist already.\n","\n","    Args:\n","        lh_name (str): The name of the lakehouse to create.\n","\n","    Returns:\n","        str: The ID of the lakehouse.\n","    \"\"\"\n","    try:\n","        # Use the fabric API to create a lakehouse and return its ID\n","        return fabric.create_lakehouse(lh_name)\n","    except FabricHTTPException as exc:\n","        # If the lakehouse already exists, return its ID\n","        return get_lakehouse_id(lh_name)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"712bd16a-afdb-4098-83e0-1a1a6cf0a10d"},{"cell_type":"markdown","source":["# Create Mount Point"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"dd832e8a-082b-4b6b-a203-91f9bbfd1f01"},{"cell_type":"code","source":["def create_mount_point(abfss_path: str, mount_point: str = \"/lakehouse/default\") -> str:\n","    \"\"\"Creates a mount point for an Azure Blob Storage path and returns the local path.\n","\n","    Args:\n","        abfss_path (str): The Azure Blob Storage path to mount.\n","        mount_point (str, optional): The mount point to use. Defaults to \"/lakehouse/default\".\n","\n","    Returns:\n","        str: The local path of the mount point.\n","\n","    Raises:\n","        ValueError: If the mount point is already in use or invalid.\n","    \"\"\"\n","    \n","    # Check if the mount point exists in fs.mounts\n","    if any(m.mountPoint == mount_point for m in mssparkutils.fs.mounts()):\n","        # Return the local path of the existing mount point\n","        return next(m.localPath for m in mssparkutils.fs.mounts() if m.mountPoint == mount_point)\n","    else:\n","        # Mount the One Lake path\n","        mssparkutils.fs.mount(abfss_path, mount_point)\n","\n","        # Return the local path of the new mount point\n","        return next(m.localPath for m in mssparkutils.fs.mounts() if m.mountPoint == mount_point)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"24c41761-4386-4797-89bc-500d888e688e"},{"cell_type":"markdown","source":["# Get Lakehouse path"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"db2b24b9-cfb3-4bdc-a62d-f00e544feec4"},{"cell_type":"code","source":["def get_lakehouse_path(lakehouse_name: str, path_type: str = \"spark\", folder_type: str = \"Tables\") -> str:\n","    \"\"\"Returns the path to a lakehouse folder based on the lakehouse name, path type, and folder type.\n","\n","    Parameters:\n","    lakehouse_name (str): The name of the lakehouse.\n","    path_type (str): The type of the path, either \"spark\" or \"local\". Defaults to \"spark\".\n","    folder_type (str): The type of the folder, either \"Tables\" or \"Files\". Defaults to \"Tables\".\n","\n","    Returns:\n","    str: The path to the lakehouse folder.\n","\n","    Raises:\n","    ValueError: If the lakehouse name, path type, or folder type is invalid.\n","    \"\"\"\n","    # Validate the parameters\n","    if not lakehouse_name:\n","        raise ValueError(\"Lakehouse name cannot be empty.\")\n","    if path_type not in [\"spark\", \"local\"]:\n","        raise ValueError(f\"Invalid path type: {path_type}.\")\n","    if folder_type not in [\"Tables\", \"Files\"]:\n","        raise ValueError(f\"Invalid folder type: {folder_type}.\")\n","\n","    # Create the lakehouse if it does not exist\n","    lakehouse_id = create_lakehouse_if_not_exists(lakehouse_name)\n","\n","    # Get the workspace id\n","    workspace_id = fabric.get_workspace_id()\n","    abfss_path = f\"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com\"\n","    abfss_lakehouse_path = os.path.join(abfss_path, lakehouse_id)\n","\n","    # Construct the path based on the path type\n","    if path_type == \"spark\":\n","        return os.path.join(abfss_lakehouse_path, folder_type)\n","    elif path_type == \"local\":\n","        local_path = create_mount_point(abfss_lakehouse_path, f\"/lakehouse/{lakehouse_name}\")\n","        return os.path.join(local_path, folder_type)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"2641edc0-51db-4f1d-a558-b561ffcb44a9"},{"cell_type":"markdown","source":["# Delete a Lakehouse item"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"2ab21b7e-b07a-4a01-8d91-e0ff6534276c"},{"cell_type":"code","source":["def delete_path(lakehouse, item, folder_type= \"Tables\"):\n","    \"\"\"Deletes the folder or file if it exists.\n","\n","    Args:\n","        path (str): The path of the folder or file to be deleted.\n","    \"\"\"\n","    path = get_lakehouse_path(lakehouse, path_type = \"spark\", folder_type = folder_type)\n","    path_item = os.path.join(path, item)\n","    if mssparkutils.fs.exists(path_item):\n","        mssparkutils.fs.rm(path_item, True)\n","    else:\n","        print(f\"Path does not exist: {path_item}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"33017d8f-e79f-4af8-b607-f063e6410888"},{"cell_type":"markdown","source":["# Download Data from URL"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"6ecb996d-da3c-447b-a7eb-6b53c9e14db6"},{"cell_type":"code","source":["# Define a function that takes the lake path and the base URL as parameters\n","def download_data(url, lakehouse, path):\n","    \"\"\"\n","    Downloads a zip file from the base URL and extracts it to the lake path.\n","\n","    Parameters\n","    ----------\n","    path : str\n","        The path of the directory where the data will be stored.\n","    url : str\n","        The URL of the zip file to be downloaded.\n","    lakehouse: str\n","        Name of Lakehouse\n","\n","    Returns\n","    -------\n","    None\n","    \"\"\"\n","    # Create a lake path\n","    lake_path = os.path.join(get_lakehouse_path(lakehouse, \"local\", \"Files\"), path)\n","\n","    # Create a file name from the base URL\n","    file_path = os.path.join(lake_path, os.path.basename(url))\n","\n","    # Create a directory for the lake path if it does not exist\n","    os.makedirs(lake_path, exist_ok=True)\n","\n","    # Download the data from the base URL and save the file in the path\n","    urlretrieve(url, file_path)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"636154d1-7373-4ced-ae38-de668ef3fb27"},{"cell_type":"markdown","source":["# Check if Delta Table Exists"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"3141859f-27ac-45c9-9e06-e4f0e6122ca8"},{"cell_type":"code","source":["def delta_table_exists(lakehouse_name: str, tbl: str) -> bool:\n","  \"\"\"Check if a delta table exists at the given path.\n","\n","  Parameters:\n","  path (str): The directory where the delta table is stored.\n","  tbl (str): The name of the delta table.\n","\n","  Returns:\n","  bool: True if the delta table exists, False otherwise.\n","  \"\"\"\n","  try:\n","    # Use the DeltaTable class to access the delta table\n","    path = get_lakehouse_path(lakehouse_name)\n","    DeltaTable.forPath(spark, os.path.join(path, tbl))\n","    # If no exception is raised, the delta table exists\n","    return True\n","  except Exception as e:\n","    # If an exception is raised, the delta table does not exist\n","    return False\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"0070d29e-62e9-41f6-94b4-0447da527bf2"},{"cell_type":"markdown","source":["# Read a Delta Table in Lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"79e78637-ee34-4361-950f-0056e057731b"},{"cell_type":"code","source":["def read_delta_table(lakehouse_name: str, table_name: str) -> DataFrame:\n","    \"\"\"Reads a delta table from a given path and table name.\n","\n","    Args:\n","        lakehouse_name (str): Name of the Lakehouse where the table exists.\n","        table_name (str): The name of the delta table.\n","\n","    Returns:\n","        pyspark.sql.DataFrame: A Spark DataFrame with the delta table data.\n","    \"\"\"\n","    path = get_lakehouse_path(lakehouse_name)\n","    return (\n","        spark\n","        .read\n","        .format('delta')\n","        .load(f\"{path}/{table_name}\")\n","    )"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"69848917-63c0-453e-819f-dc712cebe98a"},{"cell_type":"markdown","source":["# Create or Replace Delta Table"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"d447c9be-6bf9-4d50-94ec-36b5accb65e9"},{"cell_type":"code","source":["def create_or_replace_delta_table(df: DataFrame, lakehouse_name: str, table_name: str, mode_type: str = \"overwrite\") -> None:\n","    \"\"\"Create or replace a delta table from a dataframe.\n","\n","    Args:\n","        df (pyspark.sql.DataFrame): The dataframe to write to the delta table.\n","        lakehouse_name (str): Lakehouse where delta table is stored.\n","        table_name (str): The name of the delta table.\n","        mode_type (str, optional): The mode for writing the delta table. Defaults to \"overwrite\".\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    path = get_lakehouse_path(lakehouse_name)\n","    (\n","        df\n","        .write\n","        .mode(mode_type)\n","        .option(\"mergeSchema\", \"true\")\n","        .format(\"delta\")\n","        .save(f'{path}/{table_name}')\n","    )"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"da334ac2-a3d7-42e5-82a7-bd8d2d3fd1d0"},{"cell_type":"markdown","source":["# Upsert Delta table"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"df787be3-9885-4ef6-afbd-63bbe52e8af5"},{"cell_type":"code","source":["def upsert_delta_table(lakehouse_name: str, table_name: str, df: DataFrame, merge_condition: str, update_condition: dict = None) -> None:\n","    \"\"\"Updates or inserts rows into a delta table with the given data frame and conditions.\n","\n","    Args:\n","        lakehouse_name (str): The name of the lakehouse.\n","        table_name (str): The name of the delta table.\n","        df (pyspark.sql.DataFrame): The data frame to merge with the delta table.\n","        merge_condition (str): The condition to match the rows for merging.\n","        update_condition (dict, optional): The dictionary of column names and values to update when matched. Defaults to None.\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    table_path = get_lakehouse_path(lakehouse_name)\n","    delta_table = DeltaTable.forPath(spark, os.path.join(table_path, table_name))\n","    if update_condition is None:\n","        # If update_condition is None, just insert new rows when not matched\n","        (\n","            delta_table.alias('t')\n","            .merge(df.alias('s'), merge_condition)\n","            .whenNotMatchedInsertAll()\n","            .execute()\n","        )\n","    else:\n","        # Otherwise, update existing rows when matched and insert new rows when not matched\n","        (\n","            delta_table.alias('t')\n","            .merge(df.alias('s'), merge_condition)\n","            .whenMatchedUpdate(set = update_condition)\n","            .execute()\n","        )\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"f4322423-1071-4f2e-8e76-e6a779f64fee"},{"cell_type":"markdown","source":["# Create or Upsert data to Delta table"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"11a13b2d-9ce4-40bd-8272-4416387ab216"},{"cell_type":"code","source":["def create_or_insert_table(df: DataFrame, lakehouse_name: str, table_name: str, primary_key: str, merge_key: str) -> None:\n","    \"\"\"Create or insert a delta table from a dataframe.\n","\n","    Args:\n","        df (DataFrame): The dataframe to be inserted or used to create the table.\n","        lakehouse_name (str): The name of the lakehouse where the table is located.\n","        table_name (str): The name of the table to be created or inserted.\n","        primary_key (str): The name of the column that serves as the primary key of the table.\n","        merge_key (str): The name of the column that serves as the merge key of the table.\n","    \"\"\"\n","    if delta_table_exists(lakehouse_name, table_name):\n","        # Get the maximum value of the primary key from the existing table\n","        max_primary_key = read_delta_table(lakehouse_name, table_name).agg(F.max(primary_key)).collect()[0][0] + 1\n","        # Increment the primary key of the dataframe by the maximum value\n","        df = df.withColumn(primary_key, F.col(primary_key) + F.lit(max_primary_key))\n","        # Format the merge condition with the merge key\n","        merge_condition = f\"t.{merge_key} = s.{merge_key}\"\n","        # Upsert the dataframe into the existing table\n","        upsert_delta_table(lakehouse_name, table_name, df, merge_condition)\n","    else:\n","        # Create a new table from the dataframe\n","        create_or_replace_delta_table(df, lakehouse_name, table_name)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"a5a2cd7d-e30c-4093-bec4-f4a1f9fd5c9f"},{"cell_type":"markdown","source":["# Get row count of a table"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0ba86f03-84dc-447d-b944-d0301bf14272"},{"cell_type":"code","source":["def get_row_count(lakehouse_or_link: str, table_name: str = None) -> int:\n","    \"\"\"Get the row count from a delta table or a web page.\n","\n","    Args:\n","        table_path: The path of the table or web page.\n","        table_name: The name of the table. Defaults to None.\n","\n","    Returns:\n","        The row count as an integer.\n","    \"\"\"\n","    # If the table name is None, assume the table path is a web page\n","    if table_name is None:\n","        # Extract the row count from the HTML table\n","        try:\n","            row_count = int(\n","                pd\n","                .read_html(lakehouse_or_link)[1]\n","                .All\n","                .str\n","                .extract(\"([\\\\d,]+)\")\n","                .iloc[0, 0]\n","                .replace(\",\", \"\")\n","            )\n","        except ValueError:\n","            print(\"Invalid row count in the web page\")\n","            return\n","    else:\n","        # Get the row count from the delta table\n","        row_count = read_delta_table(lakehouse_or_link, table_name).count()\n","    \n","    return row_count"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"a4f7a5cb-6fb0-4cb9-9b1a-67b37632da25"},{"cell_type":"markdown","source":["# Compare row count"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"eb7290ed-b91d-4b0f-9d09-ffc6c095c13b"},{"cell_type":"code","source":["def compare_row_count(table1_lakehouse: str, table1_name: str, table2_lakehouse: str, table2_name: str = None) -> None:\n","    \"\"\"Compare the row count of two tables and exit or print the difference.\n","\n","    This function compares the row count of two delta tables or a delta table and a web page.\n","    It exits the notebook with message \"No new data\" if the row counts are equal, or prints the difference otherwise.\n","\n","    Args:\n","        table1_lakehouse: The lakehouse of the first table.\n","        table1_name: The name of the first table.\n","        table2_lakehouse: The lakehouse of the second table or web page.\n","        table2_name: The name of the second table. Defaults to None.\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    \n","    # Check if the first table exists as a delta table\n","    if delta_table_exists(table1_lakehouse, table1_name):\n","        # Get the row count and the match IDs from the first table\n","        row_count_1 = get_row_count(table1_lakehouse, table1_name)\n","        \n","        # Get the row count from the second table or web page\n","        row_count_2 = get_row_count(table2_lakehouse, table2_name)\n","        \n","        # Compare the row counts and exit or print accordingly\n","        if row_count_1 == row_count_2:\n","            # If the row counts are equal, exit the notebook with message \"No new data\"\n","            mssparkutils.notebook.exit(\"No new data\")\n","        else:\n","            print(f\"Cricsheet has {row_count_2 - row_count_1} more matches added\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"c5f39556-de5c-4feb-bb0e-39efeb24c634"},{"cell_type":"markdown","source":["# Unzip files from an archive"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"118b692e-f53b-4afd-a14e-cb26d4b63495"},{"cell_type":"code","source":["def unzip_files(zip_filename: str, filenames: list[str], path: str) -> None:\n","    \"\"\"Unzip a batch of files from a zip file to a given path.\n","\n","    Args:\n","        zip_filename (str): The name of the zip file.\n","        filenames (list[str]): The list of filenames to unzip.\n","        path (str): The destination path for the unzipped files.\n","    \"\"\"\n","    # Open the zip file\n","    with ZipFile(zip_filename, 'r') as handle:\n","        # Unzip a batch of files\n","        handle.extractall(path=path, members=filenames)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"473069ca-ae3c-4cd5-911a-3bc7a48ed1a1"},{"cell_type":"markdown","source":["# Unzip a large number of files in parallel"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1ec3357f-6273-4fe8-ac21-e7d3709f8dcb"},{"cell_type":"code","source":["def unzip_parallel(lakehouse: str, path: str, filename: str, file_type: str = None) -> None:\n","    \"\"\"Unzip all files from a zip file to a given path in parallel.\n","\n","    Args:\n","        lakehouse (str): Name of the Lakehouse\n","        path (str): The destination path for the unzipped files.\n","        filename (str): The name of the zip file without the path.\n","        file_type (str): The file type to extract. Defaults to None.\n","    \"\"\"\n","    # Create a lake path\n","    lake_path = os.path.join(get_lakehouse_path(lakehouse, \"local\", \"Files\"), path)\n","    # join the path and the filename to get the full zip file path\n","    zip_filename = os.path.join(lake_path, filename)\n","    # check if the zip file exists\n","    if os.path.exists(zip_filename):\n","        # open the zip file\n","        with ZipFile(zip_filename, 'r') as handle:\n","            # list of all files to unzip\n","            files = handle.namelist()\n","        # filter the files by file type if not None\n","        if file_type is not None:\n","            files = [f for f in files if f.endswith(file_type)]\n","        # determine chunksize\n","        n_workers = 80\n","        chunksize = round(len(files) / n_workers)\n","        # start the thread pool\n","        with ProcessPoolExecutor(n_workers) as exe:\n","            # split the copy operations into chunks\n","            for i in range(0, len(files), chunksize):\n","                # select a chunk of filenames\n","                filenames = files[i:(i + chunksize)]\n","                # submit the batch copy task\n","                _ = exe.submit(unzip_files, zip_filename, filenames, lake_path)\n","        # remove the zip file\n","        os.remove(zip_filename)\n","    else:\n","        # print an error message\n","        print(f\"The zip file {zip_filename} does not exist.\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"9df5a6c2-3973-4508-b3b9-45316f039ed6"},{"cell_type":"markdown","source":["# Get first team to Bat or Field"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"66e8c961-35f0-4077-a4b4-385d0505ea4c"},{"cell_type":"code","source":["def first_team(toss_decision, team_player_schema):\n","  \"\"\"\n","  Returns the name of the first team to play based on the toss decision.\n","\n","  Parameters:\n","  toss_decision (str): The decision of the toss winner to bat or field.\n","\n","  Returns:\n","  str: The name of the first team to play.\n","  \"\"\"\n","  # Extract the team names from the JSON column \"team_players\" using the schema \"team_player_schema\"\n","  teams = F.map_keys(F.from_json(F.col(\"team_players\"), team_player_schema))\n","  # Assign the first and second team names to variables\n","  first_team_name = teams[0]\n","  second_team_name =  teams[1]\n","  # Return the name of the first team to play based on the toss decision and the toss winner\n","  return (F.when(F.col(\"toss_decision\") == toss_decision, F.col(\"toss_winner\"))\n","          .when((F.col(\"toss_winner\") == first_team_name), second_team_name)\n","          .otherwise(first_team_name))\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"bda6cd02-2eb5-405f-9dda-7b81249665a1"},{"cell_type":"markdown","source":["# Convert a string of refresh items to json"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"39157ade-ac3d-432b-b63d-52073a5bffec"},{"cell_type":"code","source":["def convert_to_json(refresh_objects: str) -> list:\n","    \"\"\"Converts a string of refresh objects to a list of dictionaries.\n","\n","    Args:\n","        refresh_objects (str): A string of refresh objects, separated by \"|\".\n","            Each refresh object consists of a table name and optional partitions, separated by \":\".\n","            Partitions are comma-separated. \n","            eg. \"Table1:Partition1,Partition2|Table2\"\n","\n","    Returns:\n","        list: A list of dictionaries, each with a \"table\" key and an optional \"partition\" key.\n","    \"\"\"\n","    result = [] # Initialize an empty list to store the converted dictionaries\n","    if refresh_objects is None or refresh_objects == \"All\":\n","        return result # Return an empty list if the input is None or \"All\"\n","    for item in refresh_objects.split(\"|\"): # Loop through each refresh object, separated by \"|\"\n","        table, *partitions = item.split(\":\") # Split the item by \":\" and assign the first element to table and the rest to partitions\n","        if partitions: # If there are any partitions\n","            # Extend the result list with a list comprehension that creates a dictionary for each partition\n","            # The dictionary has the table name and the partition name as keys\n","            # The partition name is stripped of any leading or trailing whitespace\n","            result.extend([{\"table\": table, \"partition\": partition.strip()} for partition in \",\".join(partitions).split(\",\")])\n","        else: # If there are no partitions\n","            # Append a dictionary with only the table name as a key to the result list\n","            result.append({\"table\": table})\n","    return result # Return the final list of dictionaries\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"f3f6a044-44d0-4d8c-aaff-2f152ad56621"},{"cell_type":"markdown","source":["# Call Enhanced Refresh API"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"135b1fb1-c2cc-4722-85c6-f5bcbe9051fe"},{"cell_type":"code","source":["def start_enhanced_refresh(\n","    dataset_name: str,\n","    workspace_name: str = fabric.get_workspace_id(),\n","    refresh_objects: str = \"All\",\n","    refresh_type: str = \"full\",\n","    commit_mode: str = \"transactional\",\n","    max_parallelism: int = 10,\n","    retry_count: int = 0,\n","    apply_refresh_policy: bool = False,\n","    effective_date: datetime.date = datetime.date.today(),\n",") -> str:\n","    \"\"\"Starts an enhanced refresh of a dataset.\n","\n","    Args:\n","        dataset_name: The name of the dataset to refresh.\n","        workspace_name: The name of the workspace where the dataset is located. Defaults to the current workspace.\n","        refresh_objects: The objects to refresh in the dataset. Can be \"All\" or a list of object names. Defaults to \"All\".\n","        refresh_type: The type of refresh to perform. Can be \"full\" or \"incremental\". Defaults to \"full\".\n","        commit_mode: The commit mode to use for the refresh. Can be \"transactional\" or \"streaming\". Defaults to \"transactional\".\n","        max_parallelism: The maximum number of parallel threads to use for the refresh. Defaults to 10.\n","        retry_count: The number of times to retry the refresh in case of failure. Defaults to 0.\n","        apply_refresh_policy: Whether to apply the refresh policy defined in the dataset. Defaults to False.\n","        effective_date: The date to use for the refresh. Defaults to today.\n","\n","    Returns:\n","        The refresh request id.\n","\n","    Raises:\n","        FabricException: If the refresh fails or encounters an error.\n","    \"\"\"\n","    objects_to_refresh = convert_to_json(refresh_objects)\n","    return fabric.refresh_dataset(\n","        workspace=workspace_name,\n","        dataset=dataset_name,\n","        objects=objects_to_refresh,\n","        refresh_type=refresh_type,\n","        max_parallelism=max_parallelism,\n","        commit_mode=commit_mode,\n","        retry_count=retry_count,\n","        apply_refresh_policy=apply_refresh_policy,\n","        effective_date=effective_date,\n","    )"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"a5a44c70-5c67-4129-b2fe-ebe263803d11"},{"cell_type":"markdown","source":["# Get Enhanced Refresh Details"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"d50220af-e9d5-46fa-920e-331c13e54f9f"},{"cell_type":"code","source":["def get_enhanced_refresh_details(dataset_name: str, refresh_request_id: str, workspace_name: str = fabric.resolve_workspace_name(fabric.get_workspace_id())) -> pd.DataFrame:\n","    \"\"\"Get enhanced refresh details for a given dataset and refresh request ID.\n","\n","    Args:\n","        dataset_name (str): The name of the dataset.\n","        refresh_request_id (str): The ID of the refresh request.\n","        workspace_name (str, optional): The name of the workspace. Defaults to name workspace where Notebook reside.\n","\n","    Returns:\n","        pd.DataFrame: A dataframe with the refresh details, messages, and time taken in seconds.\n","    \"\"\"\n","    # Get the refresh execution details from fabric\n","    refresh_details = fabric.get_refresh_execution_details(workspace=workspace_name, dataset=dataset_name, refresh_request_id=refresh_request_id)\n","    \n","    # Create a dataframe with the refresh details\n","    df = pd.DataFrame({\n","        'workspace': [workspace_name],\n","        'dataset': [dataset_name],\n","        'start_time': [refresh_details.start_time],\n","        'end_time': [refresh_details.end_time if refresh_details.end_time is not None else None],\n","        'status': [refresh_details.status],\n","        'extended_status': [refresh_details.extended_status],\n","        'number_of_attempts': [refresh_details.number_of_attempts]\n","    })\n","\n","    # Calculate the time taken in seconds and add it as a new column\n","    df['duration_in_sec'] = (df['end_time'].fillna(df['start_time']) - df['start_time']).dt.total_seconds()\n","    df['duration_in_sec'] = df['duration_in_sec'].astype(int)\n","    \n","    # Convert the start_time and end_time columns to strings with the date format\n","    df['start_time'] = df['start_time'].astype(str).str.slice(0, 16)\n","    df['end_time'] = df['end_time'].astype(str).str.slice(0, 16)\n","    \n","    # Create a dataframe with the refresh messages\n","    df_msg = pd.DataFrame(refresh_details.messages)\n","    \n","    # Add the dataset column to df_msg\n","    df_msg = df_msg.assign(dataset=dataset_name)\n","    \n","    # Merge the dataframes on the dataset column and return the result\n","    return df.merge(df_msg, how='outer', on='dataset')\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"2852c114-236b-4b18-85a6-a2b9ea0aaa94"},{"cell_type":"markdown","source":["# Cancel Enhanced Refresh"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"89705901-84da-4cdb-8e38-224adbe6e4b6"},{"cell_type":"code","source":["def cancel_enhanced_refresh(request_id: str, dataset_id: str, workspace_id: str = fabric.get_workspace_id()) -> dict:\n","    \"\"\"Cancel an enhanced refresh request for a Power BI dataset.\n","\n","    Args:\n","        request_id (str): The ID of the refresh request to cancel.\n","        dataset_id (str): The ID of the dataset to cancel the refresh for.\n","        workspace_id (str, optional): The ID of the workspace containing the dataset. Defaults to the current workspace.\n","\n","    Returns:\n","        dict: The JSON response from the Power BI REST API.\n","\n","    Raises:\n","        FabricHTTPException: If the request fails with a non-200 status code.\n","    \"\"\"\n","    \n","    # Create a Power BI REST client object\n","    client = fabric.PowerBIRestClient()\n","\n","    # Construct the endpoint URL for the delete request\n","    endpoint = f\"v1.0/myorg/groups/{workspace_id}/datasets/{dataset_id}/refreshes/{request_id}\"\n","\n","    # Send the delete request and get the response\n","    response = client.delete(endpoint)\n","\n","    # Check the status code and raise an exception if not 200\n","    if response.status_code != 200:\n","        raise FabricHTTPException(response)\n","\n","    # Return the JSON response as a dictionary\n","    return response.json()\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"261562f9-8e57-4cdb-bfb1-429f0f16551c"},{"cell_type":"markdown","source":["# Check if a Semantic Model exists in the workspace"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1aed285a-8da0-434b-a0f2-9daf91b5751f"},{"cell_type":"code","source":["def is_dataset_exists(dataset: str, workspace: str = fabric.get_workspace_id()) -> bool:\n","    \"\"\"Check if a dataset exists in a given workspace.\n","\n","    Args:\n","        dataset (str): The name of the dataset to check.\n","        workspace (str, optional): The ID or Name of the workspace to search in. Defaults to the current workspace.\n","\n","    Returns:\n","        bool: True if the dataset exists, False otherwise.\n","    \"\"\"\n","    return not fabric.list_datasets(workspace).query(f\"`Dataset Name` == '{dataset}'\").empty\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"3c88f271-c93c-4973-8849-c4725372df0d"},{"cell_type":"markdown","source":["# Synchronous Enhanced refresh of datasets"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"04f5a8a0-be11-407b-8000-be448aef52ab"},{"cell_type":"code","source":["def refresh_and_wait(\n","    dataset_list: list[str],\n","    workspace: str = fabric.get_workspace_id(),\n","    logging_lakehouse: str = None,\n","    parent_job_name: str = None,\n","    job_category: str = \"Adhoc\",\n",") -> None:\n","    \"\"\"\n","    Waits for enhanced refresh of given datasets.\n","\n","    Args:\n","      dataset_list: List of datasets to refresh.\n","      workspace: The workspace ID where the datasets are located. Defaults to the current workspace.\n","      logging_lakehouse: The name of the lakehouse where the job information will be logged. Defaults to None.\n","      parent_job_name: The name of the parent job that triggered the refresh. Defaults to None.\n","\n","    Returns:\n","      None\n","\n","    Raises:\n","      Exception: If any of the datasets failed to refresh.\n","    \"\"\"\n","\n","    # Filter out the datasets that do not exist\n","    valid_datasets = [\n","                dataset\n","                for dataset in dataset_list\n","                if is_dataset_exists(dataset, workspace)\n","            ]\n","\n","    # Start the enhanced refresh for the valid datasets\n","    request_ids = {\n","        dataset: start_enhanced_refresh(dataset, workspace)\n","        for dataset in valid_datasets\n","    }\n","\n","    # Check if logging_lakehouse has value\n","    if logging_lakehouse:\n","        # Loop through the request_ids dictionary\n","        for dataset, request_id in request_ids.items():\n","            # Log entries in logging table\n","            insert_or_update_job_table(\n","                lakehouse_name=logging_lakehouse,\n","                job_name=dataset,\n","                parent_job_name=parent_job_name,\n","                request_id=request_id,\n","                job_category=job_category,\n","            )\n","\n","    # Print the datasets that do not exist\n","    invalid_datasets = set(dataset_list) - set(valid_datasets)\n","    if invalid_datasets:\n","        print(f\"The following datasets do not exist: {', '.join(invalid_datasets)}\")\n","\n","    # Loop until all the requests are completed\n","    request_status_dict = {} # Initialize an empty dictionary to store the request status of each dataset\n","    while True:\n","        for dataset, request_id in request_ids.copy().items():\n","            # Get the status and details of the current request\n","            request_status_df = get_enhanced_refresh_details(dataset, request_id, workspace)\n","            request_status = request_status_df[\"status\"].iloc[0]\n","\n","            # If the request is not unknown, print the details, store the status in the dictionary, and remove it from the request_ids\n","            if request_status != \"Unknown\":\n","                if logging_lakehouse:\n","                    duration = request_status_df[\"duration_in_sec\"].iloc[0]\n","                    msg = request_status_df[\"Message\"].iloc[0]\n","                    msg = None if pd.isna(msg) else msg\n","                    insert_or_update_job_table(\n","                        lakehouse_name=logging_lakehouse,\n","                        job_name=dataset,\n","                        parent_job_name=parent_job_name,\n","                        request_id=request_id,\n","                        status=request_status,\n","                        duration=duration,\n","                        job_category=job_category,\n","                        message=msg,\n","                    )\n","                print(request_status_df.to_markdown())\n","                request_status_dict[dataset] = request_status  # Store the status in the dictionary\n","                del request_ids[dataset]\n","\n","        # If there are no more requests, exit the loop\n","        if not request_ids:\n","            # Check if any of the datasets failed to refresh\n","            failed_datasets = [\n","                dataset\n","                for dataset, status in request_status_dict.items()\n","                if status == \"Failed\"\n","            ]\n","            # If there are any failed datasets, raise an exception with the list of them\n","            if failed_datasets:\n","                raise Exception(f\"The following datasets failed to refresh: {', '.join(failed_datasets)}\")\n","            break  # Exit the loop\n","        # Otherwise, wait for 30 seconds before checking again\n","        else:\n","            time.sleep(30)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"ba7acd6a-c4b8-40ae-88f9-8e23cc3cb4c2"},{"cell_type":"markdown","source":["# Get Job ID"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"1807ee06-de94-4188-bdd2-97a6730cf05a"},{"cell_type":"code","source":["def get_job_id(lakehouse = None, table = None, job_name = None) -> str:\n","    \"\"\"Returns a job id based on the type and the delta table.\n","\n","    Args:\n","        lakehouse (str, optional): The lakehouse of the delta table. Defaults to None.\n","        table (str, optional): The name of the delta table. Defaults to None.\n","        job_name (str, optional): The name of the job. Defaults to None.\n","        type (str, optional): The type of the job id. Can be \"Random\" or \"Latest\" or \"Latest Parent\". Defaults to \"Random\".\n","\n","    Returns:\n","        str: The job id as a string.\n","    \"\"\"\n","    job_id = str(uuid.uuid4()) # default to random job id\n","    if lakehouse and table and job_name:\n","        try:\n","            job_id = read_delta_table(lakehouse, table).filter(F.col(\"job_name\") == job_name).orderBy(F.desc(\"start_time\")).first()[\"job_id\"]\n","        except Exception as e:\n","            print(f\"Returning a random id because of this error: {e}\")\n","            pass # ignore any errors\n","    return job_id\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"274bfc26-562e-4049-803b-6445ee4a03d0"},{"cell_type":"markdown","source":["# Insert or Update Log table"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"02a3f98b-268a-4df6-ad5f-c28567097db9"},{"cell_type":"code","source":["def insert_or_update_job_table(\n","    lakehouse_name: str,\n","    job_name: str,\n","    job_category: str = \"Adhoc\",\n","    status: str = \"InProgress\",\n","    parent_job_name: str = None,\n","    request_id: str = None,\n","    message: str = None,\n","    duration: int = None,\n",") -> None:\n","    \"\"\"\n","    Inserts or updates a row in the job table with the given parameters.\n","\n","    Args:\n","        lakehouse_name: The name of Lakehouse where job table has to be stored.\n","        job_name: The name of the job.\n","        job_category: Category of a job. Defaults to Adhoc\n","        parent_job_name: Name of the parent job. Defaults to None.\n","        request_id: The request ID for the job. If None, a random job ID will be generated. Defaults to None.\n","        status: The status of the job. If \"InProgress\", the job will be inserted as a new row in the table, otherwise updated. Defaults to \"InProgress\".\n","        message: Captures error message or other messaages if any\n","    \"\"\"\n","    # Check if lakehouse_name is None and return None if it is\n","    if lakehouse_name is None:\n","        return None\n","\n","    table_name = \"t_job\"\n","    # Get the job ID and the parent job ID\n","    job_id = request_id or (\n","        get_job_id()\n","        if status == \"InProgress\"\n","        else get_job_id(lakehouse_name, table_name, job_name)\n","    )\n","    parent_job_id = (\n","        get_job_id(lakehouse_name, table_name, parent_job_name)\n","        if parent_job_name\n","        else None\n","    )\n","\n","    # Create a dataframe with the job details\n","    job_df = spark.createDataFrame(\n","        [\n","            (\n","                job_id,\n","                parent_job_id,\n","                job_name,\n","                job_category,\n","                datetime.datetime.today(),\n","                None,\n","                status,\n","                message,\n","            )\n","        ],\n","        schema=\"job_id string, parent_job_id string, job_name string, job_category string, start_time timestamp, end_time timestamp, status string, message string\",\n","    )\n","\n","    # Insert or update the job table based on the status column\n","    if status == \"InProgress\":\n","        create_or_replace_delta_table(job_df, lakehouse_name, table_name, \"append\")\n","    else:\n","        job_df = job_df.withColumn(\"end_time\", F.lit(datetime.datetime.today()))\n","        end_time = f\"t.start_time + INTERVAL {duration} SECONDS\" if duration else \"s.end_time\"\n","        merge_condition = \"s.job_id = t.job_id\"\n","        update_condition = {\n","            \"end_time\": end_time,\n","            \"status\": \"s.status\",\n","            \"message\": \"s.message\",\n","        }\n","        upsert_delta_table(lakehouse_name, table_name, job_df, merge_condition, update_condition)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"7d1a82c3-33a7-428c-b0e7-67b201b83991"},{"cell_type":"markdown","source":["# Execute DAG"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"8dd48afe-03ea-4f29-92c1-1dec5116c006"},{"cell_type":"code","source":["def execute_dag(dag):\n","    \"\"\"Run multiple notebooks in parallel and return the results or raise an exception.\"\"\"\n","    results = mssparkutils.notebook.runMultiple(dag)\n","    errors = [] # A list to store the errors\n","    for job, data in results.items(): # Loop through the dictionary\n","        if data[\"exception\"]: # Check if there is an exception for the job\n","            errors.append(f\"{job}: {data['exception']}\") # Add the error message to the list\n","    if errors: # If the list is not empty\n","        raise Exception(\"\\n\".join(errors)) # Raise an exception with the comma-separated errors\n","    else:\n","        return results"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"00cd2b56-d40f-4cf5-a5ce-84e3a462dc6f"},{"cell_type":"markdown","source":["# Execute and Log"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"5f8c6568-f8d2-4665-862c-2419ce6cdbf4"},{"cell_type":"code","source":["def execute_and_log(function: callable, log_lakehouse: str = None, job_name: str = None, job_category: str = None, parent_job_name: str = None, request_id: str = None, **kwargs) -> any:\n","    \"\"\"Executes a given function and logs the result to a lakehouse table.\n","\n","    Args:\n","        function: The function to execute.\n","        log_lakehouse: The name of the lakehouse to log to.\n","        job_name: The name of the job to log.\n","        job_category: Category of a job\n","        parent_job_name: The name of the parent job, if any.\n","        request_id: The request ID for the job, if any.\n","        **kwargs: The keyword arguments to pass to the function.\n","\n","    Returns:\n","        The result of the function execution.\n","\n","    Raises:\n","        Exception: If the function execution fails.\n","    \"\"\"\n","    result = None\n","    # check if log_lakehouse is None\n","    if log_lakehouse is not None:\n","        # call the insert_or_update_job_table function\n","        insert_or_update_job_table(lakehouse_name=log_lakehouse, job_name=job_name, job_category = job_category, parent_job_name=parent_job_name, request_id = request_id)\n","    try:\n","        result = function(**kwargs) # assign the result of the function call to a variable\n","        # check if log_lakehouse is None\n","        if log_lakehouse is not None:\n","            # call the insert_or_update_job_table function\n","            insert_or_update_job_table(lakehouse_name=log_lakehouse, job_name=job_name, status=\"Completed\")\n","    except Exception as e:\n","        msg = str(e)\n","        status = 'Completed' if msg == \"No new data\" else \"Failed\"\n","        # check if log_lakehouse is None\n","        if log_lakehouse is not None:\n","            # call the insert_or_update_job_table function\n","            insert_or_update_job_table(lakehouse_name=log_lakehouse, job_name=job_name, status=status, message=msg)\n","        raise e\n","    return result\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"79d606fa-9f2d-4704-90aa-3a37aac8a2bb"},{"cell_type":"markdown","source":["# Get Tables in a Lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"89062410-2430-404f-befb-0324c013423c"},{"cell_type":"code","source":["def get_tables_in_lakehouse(lakehouse_name):\n","    \"\"\"Returns a list of tables in the given lakehouse.\n","\n","    Args:\n","        lakehouse_name (str): The name of the lakehouse.\n","\n","    Returns:\n","        list: A list of table names, or an empty list if the lakehouse does not exist or is empty.\n","    \"\"\"\n","    try:\n","        table_list = [file.name for file in mssparkutils.fs.ls(get_lakehouse_path(lakehouse_name))]\n","    except Exception as e:\n","        print(e)\n","        table_list = []\n","    return table_list"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"3f756b4e-76e7-46fa-9d9a-1e4c6ca6d127"},{"cell_type":"markdown","source":["# Optimize and Vacuum Table using Table Maintenance API"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"e126e313-fe7b-49c3-b206-4a480a7414a7"},{"cell_type":"code","source":["def optimize_and_vacuum_table_api(lakehouse_name: str, table_name: str, workspace_id: str = fabric.get_workspace_id()) -> str:\n","    \"\"\"Optimize and vacuum a table in a lakehouse.\n","\n","    Args:\n","        lakehouse_name (str): The name of the lakehouse.\n","        table_name (str): The name of the table.\n","        workspace_id (str, optional): The ID of the workspace. Defaults to fabric.get_workspace_id().\n","\n","    Returns:\n","        Optional[str]: The operation ID of the job instance, or None if an error occurs.\n","    \"\"\"\n","    client = fabric.FabricRestClient()\n","    lakehouse_id = get_lakehouse_id(lakehouse_name)\n","    payload = {\n","        \"executionData\": {\n","            \"tableName\": table_name,\n","            \"optimizeSettings\": {\n","                \"vOrder\": True\n","            },\n","            \"vacuumSettings\": {\n","                \"retentionPeriod\": \"7.01:00:00\"\n","            }\n","        }\n","    }\n","    try:\n","        response = client.post(f\"/v1/workspaces/{workspace_id}/items/{lakehouse_id}/jobs/instances?jobType=TableMaintenance\", json= payload)\n","        if response.status_code != 202:\n","            raise FabricHTTPException(response)\n","        operation_id = response.headers['Location'].split('/')[-1]\n","        return operation_id\n","    except FabricHTTPException as e:\n","        print(e)\n","        return None"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"5e6dd169-399c-460f-be7f-42777a6f49c6"},{"cell_type":"markdown","source":["# Optimize and Vacuum Table"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"09f05e2a-9846-419d-af39-0af24a7df1ee"},{"cell_type":"code","source":["def optimize_and_vacuum_table(lakehouse_name: str, table_name: str, retain_hours: int = None) -> bool:\n","  \"\"\"\n","  Optimizes and vacuums a Delta table in a lakehouse.\n","\n","  Args:\n","      lakehouse_name: Name of the lakehouse containing the table.\n","      table_name: Name of the table to optimize and vacuum.\n","      retain_hours: Optional number of hours to retain deleted data files.\n","\n","  Returns:\n","      True on success, False on failure.\n","  \"\"\"\n","\n","  full_table_name = f\"{lakehouse_name}.{table_name}\"\n","  retain_syntax = \"\"\n","  if retain_hours is not None:\n","    spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", False)\n","    retain_syntax = f\" RETAIN {retain_hours} HOURS\"\n","\n","  try:\n","    spark.sql(f\"OPTIMIZE {full_table_name}\")\n","    spark.sql(f\"VACUUM {full_table_name}{retain_syntax}\")\n","  except Exception as e:\n","    raise Exception(f\"Error optimizing and vacuuming {full_table_name}: {e}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"947420b6-fbf0-40c3-a41b-21b9ead6a2db"},{"cell_type":"markdown","source":["# Get Lakehouse Job Status"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"42614e51-dc5e-4478-af95-6199a41f9d43"},{"cell_type":"code","source":["def get_lakehouse_job_status(operation_id: str, lakehouse_name: str, workspace_id: str = fabric.get_workspace_id()) -> dict:\n","    \"\"\"Returns the status of a lakehouse job given its operation ID and lakehouse name.\n","\n","    Args:\n","        operation_id (str): The ID of the lakehouse job operation.\n","        lakehouse_name (str): The name of the lakehouse.\n","        workspace_id (str, optional): The ID of the workspace. Defaults to None.\n","\n","    Returns:\n","        dict: A dictionary containing the operation status.\n","\n","    Raises:\n","        FabricHTTPException: If the response status code is not 200.\n","    \"\"\"\n","    client = fabric.FabricRestClient()\n","    lakehouse_id = get_lakehouse_id(lakehouse_name)\n","    try:\n","        response = client.get(f\"/v1/workspaces/{workspace_id}/items/{lakehouse_id}/jobs/instances/{operation_id}\")\n","        if response.status_code in (200, 403):\n","            return response.json()\n","        else:\n","            raise FabricHTTPException(response)\n","    except FabricHTTPException as e:\n","            print(e)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"4212e5df-ddc6-4c0f-955b-197a764bc5df"},{"cell_type":"markdown","source":["# Parse items to Optimize/Vacuum"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"abcb3008-f2c1-41b8-aa27-5dcc00c31e71"},{"cell_type":"code","source":["def prepare_optimization_items(items_to_optimize_vacuum, retain_hours=None):\n","    \"\"\"\n","    Prepares a list of items for optimizing and vacuuming tables in a lakehouse.\n","\n","    Parameters:\n","    items_to_optimize_vacuum (dict or str): A dictionary with lakehouse names as keys and lists of table names as values. If a string is provided, it's evaluated to a dictionary or converted into one with a None value.\n","    retain_hours (int, optional): The number of hours to retain the data during the vacuum process. Defaults to None.\n","\n","    Returns:\n","    list: A list of tuples, each containing the lakehouse name, table name, and retain_hours for the optimization task.\n","    \"\"\"\n","    # Evaluate string input to a dictionary or encapsulate it in a dictionary\n","    if isinstance(items_to_optimize_vacuum, str):\n","        try:\n","            items_to_optimize_vacuum = ast.literal_eval(items_to_optimize_vacuum)\n","        except ValueError:\n","            items_to_optimize_vacuum = {items_to_optimize_vacuum: None}\n","    \n","    items = []\n","    # Iterate over the lakehouses and their respective tables\n","    for lakehouse_name, table_list in items_to_optimize_vacuum.items():\n","        # If no table list is provided, retrieve all tables in the lakehouse\n","        table_list = table_list or get_tables_in_lakehouse(lakehouse_name)\n","        # Ensure the table_list is a list even if a single table name is provided\n","        table_list = [table_list] if isinstance(table_list, str) else table_list\n","        # Create a task for each table\n","        for table_name in table_list:\n","            items.append((lakehouse_name, table_name, retain_hours))\n","    \n","    return items"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"4aae003c-d098-4d11-856f-9be9ac7e10c9"},{"cell_type":"markdown","source":["# Optimize and Vacuum multiple items using Table Maintenance API"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"bf60fff0-e7e6-43d3-a78f-5149ea46788f"},{"cell_type":"code","source":["def optimize_and_vacuum_items_api(items_to_optimize_vacuum: str | dict, log_lakehouse: str = None, job_category: str = None, parent_job_name: str = None, parallelism: int = 3) -> None:\n","    \"\"\"Optimize and vacuum tables in lakehouses.\n","\n","    Args:\n","        items_to_optimize_vacuum: A dictionary of lakehouse names and table lists, or a string that can be evaluated to a dictionary, or a single lakehouse name.\n","        log_lakehouse: The name of the lakehouse where the job details will be logged, if any.\n","        job_category: The category of the job, if any.\n","        parent_job_name: The name of the parent job, if any.\n","        parallelism: The number of tables to optimize and vacuum concurrently, default is 3.\n","    \"\"\"\n","    # Create a queue to store the pending tables\n","    queue = [(item[0], item[1]) for item in prepare_optimization_items(items_to_optimize_vacuum)]\n","    \n","    # Optimize and vacuum each table in each lakehouse and store the operation ids\n","    job_details = {}\n","    # Create a list to store the running tables\n","    running = []\n","    operation_status_dict = {}\n","    # Loop until the queue is empty and the running list is empty\n","    while queue or running:\n","        # If the running list is not full and the queue is not empty, pop a table from the queue and start the operation\n","        while len(running) < parallelism and queue:\n","            lakehouse_name, table_name = queue.pop(0)\n","            operation_id = optimize_and_vacuum_table_api(lakehouse_name, table_name)\n","            if log_lakehouse:\n","                insert_or_update_job_table(\n","                    lakehouse_name=log_lakehouse,\n","                    job_name=f\"{lakehouse_name}.{table_name}\",\n","                    parent_job_name=parent_job_name,\n","                    request_id=operation_id,\n","                    job_category=job_category,\n","                )\n","            job_details[lakehouse_name, table_name] = operation_id\n","            running.append((lakehouse_name, table_name, operation_id))\n","        # Check the status of the running tables and remove the ones that are finished\n","        for (lakehouse_name, table_name, operation_id) in running.copy():\n","            operation_details = get_lakehouse_job_status(operation_id, lakehouse_name)\n","            operation_status = operation_details['status']\n","            if operation_status not in [\"NotStarted\", \"InProgress\"]:\n","                running.remove((lakehouse_name, table_name, operation_id))\n","                operation_status_dict[lakehouse_name, table_name] = operation_status\n","                if log_lakehouse:\n","                    start_time = dateutil.parser.isoparse(operation_details['startTimeUtc'])\n","                    end_time = dateutil.parser.isoparse(operation_details['endTimeUtc'])\n","                    duration = (end_time - start_time).total_seconds()\n","                    msg = operation_details['failureReason']\n","                    insert_or_update_job_table(\n","                        lakehouse_name=log_lakehouse,\n","                        job_name=f\"{lakehouse_name}.{table_name}\",\n","                        parent_job_name=parent_job_name,\n","                        request_id=operation_id,\n","                        status=operation_status,\n","                        duration=duration,\n","                        job_category=job_category,\n","                        message=msg\n","                    )\n","    \n","    # Raise an exception if any table failed to optimize\n","    failed_tables = [f\"{lakehouse_name}.{table_name}\" for (lakehouse_name, table_name), status in operation_status_dict.items() if status == 'Failed']\n","    if failed_tables:\n","        raise Exception(f\"The following tables failed to optimize: {', '.join(failed_tables)}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"0ec1c6eb-fc44-4b35-b933-d0c502c0bb5e"},{"cell_type":"markdown","source":["# Optimize and Vacuum multiple items"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"93e5c4d8-ceb1-4b62-ac8b-ccc966642551"},{"cell_type":"code","source":["def optimize_and_vacuum_items(items_to_optimize_vacuum: str | dict, retain_hours: int = None, parallelism: int = 4) -> None:\n","    \"\"\"\n","    Optimizes and vacuums Delta tables in parallel across multiple lakehouses.\n","\n","    Args:\n","        items_to_optimize_vacuum: A string representing a single item (lakehouse.table)\n","                                    or a dictionary where keys are lakehouse names\n","                                    and values are lists of tables (str) or None\n","                                    to get all tables from that lakehouse.\n","        retain_hours: Optional number of hours to retain deleted data files.\n","        parallelism: Number of parallel threads to use for optimization/vacuum tasks.\n","    \"\"\"\n","\n","    # Prepare tasks\n","    tasks = prepare_optimization_items(items_to_optimize_vacuum)\n","\n","    # Use ThreadPoolExecutor for parallel execution\n","    with ThreadPoolExecutor(max_workers=parallelism) as executor:\n","        futures = []\n","        for task in tasks:\n","            futures.append(executor.submit(optimize_and_vacuum_table, *task))\n","        for future in as_completed(futures):\n","            # No explicit return value handling, potential errors handled in worker function\n","            pass\n","\n","    print(\"Optimization and vacuum completed.\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"5e77e607-e74b-4b3e-b6ab-de854f4dacb5"},{"cell_type":"markdown","source":["# Get SQL Analytics Server and DB Details of a Lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"387ad999-fc5f-44a8-aa3d-d9e3d94752b0"},{"cell_type":"code","source":["def get_server_db(lakehouse_name: str, workspace_id: str = fabric.get_workspace_id()) -> tuple:\n","    \"\"\"\n","    Retrieves the server and database details for a given lakehouse.\n","\n","    Args:\n","        lakehouse_name (str): The name of the lakehouse.\n","        workspace_id (str, optional): The workspace ID. Defaults to Current workspace id.\n","\n","    Returns:\n","        tuple: A tuple containing the SQL Analytics server connection string and database ID.\n","    \"\"\"\n","\n","    client = fabric.FabricRestClient()\n","    lakehouse_id = get_lakehouse_id(lakehouse_name)\n","    response = client.get(f\"/v1/workspaces/{workspace_id}/lakehouses/{lakehouse_id}\")\n","    \n","    if response.status_code == 200:\n","        response_json = response.json()\n","        sql_end_point = response_json['properties']['sqlEndpointProperties']\n","        server = sql_end_point['connectionString']\n","        db = sql_end_point['id']\n","        return server, db\n","    else:\n","        raise Exception(f\"Failed to get lakehouse details: {response.status_code}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"2aedabd3-e18f-427e-98ec-464cae1f0ff6"},{"cell_type":"markdown","source":["# Build a Shared Expression M code"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"87d4248b-d3b7-4713-b1a8-18b0b4789033"},{"cell_type":"code","source":["def get_shared_expression(lakehouse_name: str, workspace_id: str = fabric.get_workspace_id()) -> str:\n","    \"\"\"\n","    This function generates the shared expression statement for a given lakehouse and its SQL endpoint.\n","\n","    Args:\n","        lakehouse_name (str): An optional parameter to set the lakehouse. This defaults to the lakehouse attached to the notebook.        \n","        workspace_id (str, optional): An optional parameter to set the workspace in which the lakehouse resides. This defaults to the workspace in which the notebook resides.\n","\n","    Returns:\n","        This function returns an M statement which can be used as the expression in the shared expression for a Direct Lake semantic model.\n","    \"\"\"\n","\n","    server, db = get_server_db(lakehouse_name, workspace_id)\n","    m_statement = 'let\\n\\tdatabase = Sql.Database(\"' + server + '\", \"' + db + '\")\\nin\\n\\tdatabase'\n","    return m_statement"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"f146ebda-9a13-4021-88f6-bb3ff10a6685"},{"cell_type":"markdown","source":["# Repoint Semantic Model to a Lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0a914169-ea34-4cb8-8fea-b6c32894f20b"},{"cell_type":"code","source":["def update_model_expression(dataset_name: str, lakehouse_name: str, workspace_id: str = fabric.get_workspace_id()) -> None:\n","    \"\"\"\n","    Update the expression in the semantic model to point to the specified lakehouse.\n","\n","    Args:\n","    - dataset_name (str): The name of the dataset.\n","    - lakehouse_name (str): The name of the lakehouse.\n","    - workspace_id (str, optional): An optional parameter to set the workspace in which the lakehouse resides. This defaults to the workspace in which the notebook resides.\n","    \"\"\"\n","    tom_server = fabric.create_tom_server(readonly = False, workspace = workspace_id)\n","    tom_database = tom_server.Databases.GetByName(dataset_name)\n","    shared_expression = get_shared_expression(lakehouse_name)\n","\n","    try:\n","        model = tom_database.Model\n","        model.Expressions['DatabaseQuery'].Expression = shared_expression\n","        model.SaveChanges()\n","        print(f\"The expression in the '{dataset_name}' semantic model has been updated to point to the '{lakehouse_name}' lakehouse.\")\n","    except Exception as e:\n","        print(f\"ERROR: The expression in the '{dataset_name}' semantic model was not updated. Error: {e}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"48c6e3a5-9ef3-48a4-9f95-1daf53e40ea0"},{"cell_type":"markdown","source":["# Encode file content to base64"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"05964c38-1a55-4b3b-928d-2bdef14fe92d"},{"cell_type":"code","source":["def encode_to_base64(file):\n","    return base64.b64encode(json.dumps(file).encode('utf-8')).decode('utf-8')"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"bf697e62-5e88-4a0e-9eb1-c9b47a1465f3"},{"cell_type":"markdown","source":["# Decode Base64 encoded data to JSON format"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"f4ec7e68-28d1-43f0-b8ba-435192858306"},{"cell_type":"code","source":["def decode_from_base64(encoded_data):\n","    # Decode the Base64 data\n","    decoded_bytes = base64.b64decode(encoded_data)\n","    # Convert bytes to string\n","    decoded_str = decoded_bytes.decode('utf-8')\n","    # Convert string to JSON\n","    decoded_json = json.loads(decoded_str)\n","    return decoded_json"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"1be90479-03d6-49fe-80d8-2034c1ed1035"},{"cell_type":"markdown","source":["# Check the status of the Fabric Operation"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"b03bf28f-be23-46c6-a3af-061c2d973771"},{"cell_type":"code","source":["def check_operation_status(operation_id, client = fabric.FabricRestClient()):\n","    operation_response = client.get(f\"/v1/operations/{operation_id}\").json()\n","    while operation_response['status'] != 'Succeeded':\n","        time.sleep(3)\n","        operation_response = client.get(f\"/v1/operations/{operation_id}\").json()\n","    print('Operation succeeded')"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"481c9e55-f989-42e1-b7f6-64ca1ab8c660"},{"cell_type":"markdown","source":["# Create or Replace Semantic Model from bim"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"01322b5b-639e-4f85-8982-ce556d90d655"},{"cell_type":"code","source":["def create_or_replace_semantic_model_from_bim(dataset_name, bim_file_json, workspace_id=fabric.get_workspace_id()):\n","    \"\"\"\n","    This function deploys a Model.bim file to create a new semantic model in a workspace. If it already exists it will overwrite it.\n","\n","    Parameters:\n","\n","        dataset_name: The name of the semantic model to be created.\n","        bim_file_json: model.bim file loaded as a json dict\n","        workspace_id: An optional parameter to set the workspace in which the lakehouse resides. This defaults to the workspace in which the notebook resides.\n","\n","    Returns:\n","\n","        This function returns a printout stating the success/failure of the operation.\n","\n","    \"\"\"\n","\n","    # Define the object type for the semantic model\n","    object_type = 'SemanticModel'\n","\n","    # Default Power BI dataset definition\n","    def_pbi_dataset = {\"version\": \"1.0\", \"settings\": {}}\n","\n","    # Prepare the request body with the dataset name and encoded BIM file content\n","    request_body = {\n","        'displayName': dataset_name,\n","        'type': object_type,\n","        'definition': {\n","            \"parts\": [\n","                {\n","                    \"path\": \"model.bim\", \n","                    \"payload\": encode_to_base64(bim_file_json), \n","                    \"payloadType\": \"InlineBase64\"\n","                },\n","                {\n","                    \"path\": \"definition.pbidataset\", \n","                    \"payload\": encode_to_base64(def_pbi_dataset), \n","                    \"payloadType\": \"InlineBase64\"\n","                }\n","            ]\n","        }\n","    }\n","\n","    # Call the function to create or replace the fabric item\n","    create_or_replace_fabric_item(workspace_id, dataset_name, object_type, request_body)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"c038b120-aa81-4ea6-af41-210e59c1a8bb"},{"cell_type":"markdown","source":["# Create or replace report from report json"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"f0a6d1a7-50a7-420b-8024-16d5ca21d986"},{"cell_type":"code","source":["def create_or_replace_report_from_reportjson(report_name, dataset_name, report_json, theme_json=None, workspace_id=fabric.get_workspace_id()):\n","    \"\"\"\n","    Create or replace a report from a report JSON definition.\n","\n","    Args:\n","        report_name (str): The name of the report.\n","        dataset_name (str): The name of the dataset.\n","        report_json (dict): The JSON definition of the report.\n","        theme_json (dict, optional): The JSON definition of the theme. Defaults to None.\n","        workspace_id (str, optional): An optional parameter to set the workspace in which the lakehouse resides. This defaults to the workspace in which the notebook resides.\n","\n","    Returns:\n","        None\n","    \"\"\"\n","\n","    # Define the object type for the report\n","    object_type = 'Report'\n","\n","    # Retrieve the semantic model associated with the dataset name\n","    dataset_df = get_fabric_items(item_name=dataset_name, item_type='SemanticModel')\n","    if dataset_df.empty:\n","        print(f\"ERROR: The '{dataset_name}' semantic model does not exist.\")\n","        return\n","\n","    # Extract the ID of the dataset\n","    dataset_id = dataset_df['Id'].iloc[0]\n","\n","    # Prepare the Power BI report definition\n","    pbir_def = {\n","        \"version\": \"1.0\",\n","        \"datasetReference\": {\n","            \"byPath\": None,\n","            \"byConnection\": {\n","                \"connectionString\": None,\n","                \"pbiServiceModelId\": None,\n","                \"pbiModelVirtualServerName\": \"sobe_wowvirtualserver\",\n","                \"pbiModelDatabaseName\": dataset_id,\n","                \"name\": \"EntityDataSource\",\n","                \"connectionType\": \"pbiServiceXmlaStyleLive\"\n","            }\n","        }\n","    }\n","\n","    # Initialize the parts list with the report definition\n","    parts = [\n","        {\n","            \"path\": \"report.json\",\n","            \"payload\": encode_to_base64(report_json),\n","            \"payloadType\": \"InlineBase64\"\n","        },\n","        {\n","            \"path\": \"definition.pbir\",\n","            \"payload\": encode_to_base64(pbir_def),\n","            \"payloadType\": \"InlineBase64\"\n","        }\n","    ]\n","\n","    # If a theme is provided, add it to the parts list\n","    if theme_json:\n","        theme_id = theme_json['payload']['blob']['displayName']\n","        theme_path = f'StaticResources/SharedResources/BaseThemes/{theme_id}.json'\n","        parts.append({\n","            \"path\": theme_path,\n","            \"payload\": encode_to_base64(theme_json),\n","            \"payloadType\": \"InlineBase64\"\n","        })\n","\n","    # Prepare the request body with the report name, type, and definition\n","    request_body = {\n","        'displayName': report_name,\n","        'type': object_type,\n","        'definition': {\"parts\": parts}\n","    }\n","\n","    # Call the function to create or replace the fabric item\n","    create_or_replace_fabric_item(workspace_id, report_name, object_type, request_body)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"043e6f00-8ef2-442e-9efe-023535287817"},{"cell_type":"markdown","source":["# Create or Replace Notebook"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"7cada12e-ffef-4fde-94ce-8c131967ac5e"},{"cell_type":"code","source":["def create_or_replace_notebook_from_ipynb(notebook_name, notebook_json, default_lakehouse_name = None, workspace_id=fabric.get_workspace_id()):\n","    \"\"\"\n","    Create or replace a notebook in the Fabric workspace.\n","\n","    This function takes a notebook name, its JSON content, and an optional workspace ID. It encodes the notebook JSON content to Base64 and sends a request to create or replace the notebook item in the specified Fabric workspace.\n","\n","    Parameters:\n","    - notebook_name (str): The display name of the notebook.\n","    - notebook_json (str): The JSON content of the notebook to be encoded to Base64.\n","    - workspace_id (str): An optional parameter to set the workspace in which the lakehouse resides. This defaults to the workspace in which the notebook resides.\n","\n","    Returns:\n","    None\n","    \"\"\"\n","    # Define the object type for the notebook.\n","    object_type = 'Notebook'\n","\n","    if default_lakehouse_name:\n","        default_lakehouse_id = get_lakehouse_id(default_lakehouse_name)\n","        new_lakehouse_data = {\"lakehouse\": {\n","        \"default_lakehouse\": default_lakehouse_id,\n","        \"default_lakehouse_name\": default_lakehouse_name,\n","        \"default_lakehouse_workspace_id\": workspace_id\n","        }}\n","\n","        # Update the 'lakehouse' key\n","        notebook_json['metadata']['dependencies'] = new_lakehouse_data\n","\n","    # Construct the request body with the notebook details.\n","    request_body = {\n","        \"displayName\": notebook_name,\n","        \"type\": object_type,\n","        \"definition\": {\n","            \"format\": \"ipynb\",\n","            \"parts\": [\n","                {\n","                    \"path\": \"artifact.content.ipynb\",\n","                    \"payload\": encode_to_base64(notebook_json),\n","                    \"payloadType\": \"InlineBase64\"\n","                }\n","            ]\n","        }\n","    }\n","\n","    # Call the function to create or replace the notebook item in the Power BI workspace.\n","    create_or_replace_fabric_item(workspace_id, notebook_name, object_type, request_body)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"fd3b9849-2372-4980-80c8-b517a3d2daef"},{"cell_type":"markdown","source":["# Create or Replace Fabric Item"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"d5869caa-fbce-494c-b91b-5e7f3ab29814"},{"cell_type":"code","source":["def create_or_replace_fabric_item(workspace_id, item_name, object_type, request_body):\n","    \"\"\"\n","    Create or replace a fabric item within a given workspace.\n","\n","    This function checks if an item with the given name and type already exists within the workspace.\n","    If it does not exist, it creates a new item. If it does exist, it replaces the existing item with the new definition.\n","\n","    Parameters:\n","    - workspace_id (str): The ID of the workspace where the item is to be created or replaced.\n","    - item_name (str): The name of the item to be created or replaced.\n","    - object_type (str): The type of the item (e.g., 'dataset', 'report').\n","    - request_body (dict): The definition of the item in JSON format.\n","\n","    Returns:\n","    - None\n","    \"\"\"\n","\n","    # Initialize the REST client for the fabric service\n","    client = fabric.FabricRestClient()\n","\n","    # Retrieve existing items of the same name and type\n","    df = get_fabric_items(item_name=item_name, item_type=object_type)\n","\n","    # If no existing item, create a new one\n","    if df.empty:\n","        response = client.post(f\"/v1/workspaces/{workspace_id}/items\", json=request_body)\n","        print(f\"'{item_name}' created as a new {object_type}.\")\n","    else:\n","        # If item exists, replace it with the new definition\n","        item_id = df.Id.values[0]\n","        print(f\"'{item_name}' already exists as a {object_type} in the workspace. Replacing it...\")\n","        response = client.post(f\"/v1/workspaces/{workspace_id}/items/{item_id}/updateDefinition\", json=request_body)\n","\n","    # Check the response status code to determine the outcome\n","    status_code = response.status_code\n","    if status_code == 200:\n","        print(\"Operation succeeded\")\n","    elif status_code in (201, 202):\n","        # If status code indicates a pending operation, check its status\n","        check_operation_status(response.headers['x-ms-operation-id'], client)\n","    else:\n","        # If operation failed, print the status code\n","        print(f\"Operation failed with status code: {status_code}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"6f18149d-666a-496b-a66d-c66185b31bb6"},{"cell_type":"markdown","source":["# Download and Replace Contents in a Notebook"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"bf5eef17-b18a-4c7d-9dd9-cd271c55611a"},{"cell_type":"code","source":["def download_and_update_notebook(url, replacements):\n","    \"\"\"Downloads a Jupyter notebook and updates specific strings in code cells.\n","\n","    Args:\n","    url: The URL of the Jupyter notebook to download.\n","    replacements: A dictionary of strings to find and their replacements.\n","\n","    Returns:\n","    The updated Jupyter notebook as a JSON object.\n","    \"\"\"\n","    ipynb_json = requests.get(url).json()\n","    \n","    for cell in ipynb_json['cells']:\n","        if cell['cell_type'] == 'code':\n","            new_source = []\n","            for line in cell['source']:\n","                for key, value in replacements.items():\n","                    if key in line:\n","                        line = line.replace(key, value)\n","                new_source.append(line)\n","            cell['source'] = new_source\n","\n","    return ipynb_json"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"eba98426-c150-4c3d-808c-b8a76feb9448"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"microsoft":{"language":"python"},"widgets":{},"kernel_info":{"name":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}