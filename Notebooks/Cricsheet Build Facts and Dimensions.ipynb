{"cells":[{"cell_type":"markdown","id":"f3765436-7fc4-492a-8db5-790dfa2a5dea","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["# Import Libraries"]},{"cell_type":"code","execution_count":null,"id":"d8aa34b2-0d4c-4096-a905-b170dfa96067","metadata":{"jupyter":{"outputs_hidden":true,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["%run \"/Cricsheet Initialize\""]},{"cell_type":"markdown","id":"ff152a77-af64-468c-8bee-075dae5e5024","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["# Set Parameters"]},{"cell_type":"code","execution_count":null,"id":"59eb2d85-a5f2-4423-9292-7221cd7dc0ce","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}},"tags":["parameters"]},"outputs":[],"source":["master_job_name = None"]},{"cell_type":"markdown","id":"2c38ff2d-4e93-4aaa-a9af-8c6235a738be","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["# Set Variables"]},{"cell_type":"code","execution_count":null,"id":"b196c969-3a98-4e06-a907-738ed2b5f91d","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Define the schema for the team_player column, which is a map of team names to player names\n","team_player_schema = T.MapType(T.StringType(), T.ArrayType(T.StringType()))\n","\n","# Define the schema for the array columns, which are arrays of strings\n","array_schema = T.ArrayType(T.StringType())\n","\n","# Unpack the paths for the six tables to be created from the clean table path\n","(\n","    match_table_path,\n","    team_table_path,\n","    player_table_path,\n","    date_table_path,\n","    team_players_table_path,\n","    deliveries_table_path,\n","    cricsheet_table_path,\n",") = (\n","    list([LAKEHOUSE] + [tbl] + [REPORTING_SCHEMA] for tbl in [\n","        \"t_dim_match\",\n","        \"t_dim_team\",\n","        \"t_dim_player\",\n","        \"t_dim_date\",\n","        \"t_fact_team_players\",\n","        \"t_fact_deliveries\",\n","    ])\n","    + [\n","        [LAKEHOUSE] + [\"t_cricsheet\"] + [STAGING_SCHEMA]\n","    ]\n",")\n","\n","# Set job category\n","job_category = \"Build Star Schema\"\n","\n","# Read the cricsheet table as a delta table\n","raw_df = D.read_delta_table(*cricsheet_table_path)"]},{"cell_type":"markdown","id":"4a773be6-2d19-4c3b-b059-e8caa5509efa","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["# Check if Cricsheet has new matches added, else Quit"]},{"cell_type":"code","execution_count":null,"id":"c37a5603-8a14-43a7-be38-ef0f8d8b1a78","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["J.execute_and_log(\n","    function=D.compare_row_count,\n","    log_lakehouse=LAKEHOUSE,\n","    table1_lakehouse=match_table_path[0],\n","    table1_name=match_table_path[1],\n","    table1_schema=match_table_path[2],\n","    table2_lakehouse=cricsheet_table_path[0],\n","    table2_name=cricsheet_table_path[1],\n","    table2_schema=cricsheet_table_path[2],\n","    job_name='Compare Row Count Gold',\n","    job_category = job_category,\n","    parent_job_name=master_job_name\n","    )"]},{"cell_type":"markdown","id":"25939f07-72b1-40fb-88d1-6ec6aa019098","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["# Create Team Player Dataframe and Cache it"]},{"cell_type":"code","execution_count":null,"id":"f55591ce-023d-4532-9722-fc54c67072d8","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["tpd = (\n","    raw_df\n","    # Select the match_id column and explode the players array from the match_info column\n","    .select(\"match_id\", \n","            F.explode(F.from_json(F.get_json_object(\"match_info\",'$.players'), team_player_schema)))\n","    # Select the match_id column, rename the key column as team, and explode the value array as player_name\n","    .select(\"match_id\",\n","            F.col(\"key\").alias(\"team\"),\n","            F.explode(\"value\").alias(\"player_name\"))\n","    # Cache the result for faster access\n","    .cache()\n",")\n","# Assign an alias to the temporary table\n","tpd = tpd.alias(\"tpd\")\n"]},{"cell_type":"markdown","id":"bb504546-cad1-4512-b0a5-d484de3f600f","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["# Create t_dim_player"]},{"cell_type":"code","execution_count":null,"id":"b7f92d34-add5-494f-b98c-fd7d34dcdcf7","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["pdf = (\n","    tpd\n","    .select(\"player_name\")\n","    .distinct() # Get distinct player name\n","    .select(F.monotonically_increasing_id().alias(\"player_id\"), \"*\") # Add a primary key column player_id with increasing IDs\n","    .union(spark.createDataFrame([[-1, \"Extras\"]])) # Append a row for extras with ID -1\n",")\n","\n","# Write the DataFrame to a Delta table\n","# Create a new table if it does not exist, or append the new payers to the existing table\n","J.execute_and_log(\n","    function=D.create_or_insert_table,\n","    df=pdf,\n","    lakehouse_name=player_table_path[0],\n","    table_name=player_table_path[1],\n","    schema_name=player_table_path[2],\n","    primary_key = \"player_id\",\n","    merge_key = \"player_name\",\n","    log_lakehouse=LAKEHOUSE,\n","    job_name=player_table_path[1],\n","    job_category = job_category,\n","    parent_job_name=master_job_name\n","    )"]},{"cell_type":"markdown","id":"b81ad866-0075-4c78-bccb-e8cbb0d123ff","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["# Create t_dim_team"]},{"cell_type":"code","execution_count":null,"id":"bc8866d6-ecd5-4ec6-bccc-69c0d175c483","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["tdf = (\n","    tpd\n","    .select(\"team\")\n","    .distinct() # Get distinct team\n","    .select(F.monotonically_increasing_id().alias(\"team_id\"), \"*\") # Add a primary key column team_id with increasing IDs\n",")\n","\n","# Write the DataFrame to a Delta table\n","# Create a new table if it does not exist, or append the new teams to the existing table\n","J.execute_and_log(\n","    function=D.create_or_insert_table,\n","    df=tdf,\n","    lakehouse_name=team_table_path[0],\n","    table_name=team_table_path[1],\n","    schema_name=team_table_path[2],\n","    primary_key = \"team_id\",\n","    merge_key = \"team\",\n","    log_lakehouse=LAKEHOUSE,\n","    job_name=team_table_path[1],\n","    job_category = job_category,\n","    parent_job_name=master_job_name\n","    )"]},{"cell_type":"markdown","id":"25c439f8-d5e6-4aaf-8e48-b95d25c7f4dc","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["# Create Dataframe aliases and functions"]},{"cell_type":"code","execution_count":null,"id":"4b51e05b-9437-465a-9b42-3ca4bd91e393","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Read the player table from the delta table and assign it to pdf\n","pdf = D.read_delta_table(*player_table_path).alias(\"pdf\")\n","\n","# Create aliases for the player table based on different roles\n","pom = pdf.alias(\"pom\") # Player of the match\n","bat = pdf.alias(\"bat\") # Batsman\n","bwl = pdf.alias(\"bwl\") # Bowler\n","fld = pdf.alias(\"fld\") # Fielder\n","nsr = pdf.alias(\"nsr\") # Non Striker\n","pot = pdf.alias(\"pot\") # Player Out\n","\n","# Read the team table from the delta table and assign it to tdf\n","tdf = D.read_delta_table(*team_table_path).alias(\"tdf\")\n","\n","# Create aliases for the team table based on different metrics\n","mwn = tdf.alias(\"mwn\") # Match Winner\n","elm = tdf.alias(\"elm\") # Match Winner by Eliminator\n","twn = tdf.alias(\"twn\") # Toss winner\n","fbt = tdf.alias(\"fbt\") # First batting team\n","flt = tdf.alias(\"flt\") # First fielding team\n","\n","def first_team(toss_decision, team_player_schema):\n","  \"\"\"\n","  Returns the name of the first team to play based on the toss decision.\n","\n","  Parameters:\n","  toss_decision (str): The decision of the toss winner to bat or field.\n","\n","  Returns:\n","  str: The name of the first team to play.\n","  \"\"\"\n","  # Extract the team names from the JSON column \"team_players\" using the schema \"team_player_schema\"\n","  teams = F.map_keys(F.from_json(F.col(\"team_players\"), team_player_schema))\n","  # Assign the first and second team names to variables\n","  first_team_name = teams[0]\n","  second_team_name =  teams[1]\n","  # Return the name of the first team to play based on the toss decision and the toss winner\n","  return (F.when(F.col(\"toss_decision\") == toss_decision, F.col(\"toss_winner\"))\n","          .when((F.col(\"toss_winner\") == first_team_name), second_team_name)\n","          .otherwise(first_team_name))"]},{"cell_type":"markdown","id":"f1a68420-cb80-4faa-a012-a4f847303e14","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["# Create t_dim_match"]},{"cell_type":"code","execution_count":null,"id":"387f23d6-9d7c-44de-a9b4-e1b0411181f5","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Define a list of fields to extract from the match_info column, which is a JSON string\n","mdf_json_fields = [\n","    [\"dates[0]\", \"date\", \"match_date\"],\n","    [\"gender\", \"string\", \"match_gender\"],\n","    [\"season\", \"string\", \"season\"],\n","    [\"event.name\", \"string\", \"event_name\"],\n","    [\"event.group\", \"string\", \"event_group\"],\n","    [\"event.match_number\", \"int\", \"event_match_number\"],\n","    [\"city\", \"string\", \"city\"],\n","    [\"venue\", \"string\", \"venue\"],\n","    [\"officials.umpires\", \"string\", \"umpires\"],\n","    [\"team_type\",\"string\", \"team_type\"],\n","    [\"match_type\", \"string\", \"match_type\"],\n","    [\"outcome.winner\", \"string\", \"match_winner\"],\n","    [\"outcome.result\", \"string\", \"match_result\"],\n","    [\"outcome.by.runs\", \"string\", \"match_won_by_runs\"],\n","    [\"outcome.by.wickets\", \"string\", \"match_won_by_wickets\"],\n","    [\"outcome.by.innings\", \"string\", \"match_won_by_innings\"],\n","    [\"outcome.eliminator\", \"string\", \"match_winner_eliminator\"],\n","    [\"outcome.method\", \"string\", \"match_result_method\"],\n","    [\"toss.winner\", \"string\", \"toss_winner\"],\n","    [\"toss.decision\", \"string\", \"toss_decision\"],\n","    [\"player_of_match[0]\", \"string\", \"player_of_match\"],\n","    [\"players\", \"string\", \"team_players\"]\n","]\n","\n","# Create a list of column expressions to select from the raw_df dataframe, using the get_json_object function to parse the match_info column\n","mdf_json_select_lst = [\n","    F.get_json_object(\"match_info\", \"$.\" + json[0]).cast(json[1]).alias(json[2])\n","    for json in mdf_json_fields\n","]\n","\n","# Create a new dataframe by selecting the match_id and the columns from the mdf_json_select_lst list\n","mdf = (\n","  raw_df\n","  .select(\"match_id\",\n","          *mdf_json_select_lst)\n","  .select(\"*\", \n","          first_team('bat', team_player_schema).alias(\"first_bat\"),\n","          first_team('field', team_player_schema).alias(\"first_field\"))\n","  .alias(\"mdf\")\n",")\n","\n","# Join the mdf dataframe with other dataframes based on the team and player names, using left outer join\n","mdf = ( \n","  mdf\n","  .join(twn, twn.team == mdf.toss_winner, 'leftouter' )\n","  .join(mwn, mwn.team == mdf.match_winner, 'leftouter' )\n","  .join(elm, elm.team == mdf.match_winner_eliminator, 'leftouter' )\n","  .join(pom, pom.player_name == mdf.player_of_match, 'leftouter' )\n","  .join(fbt, fbt.team == mdf.first_bat, 'leftouter' )\n","  .join(flt, flt.team == mdf.first_field, 'leftouter' )\n","  .select(\"mdf.match_id\",\n","          \"mdf.match_date\",\n","          \"mdf.match_gender\",\n","          \"mdf.season\",\n","          \"mdf.event_name\",\n","          \"mdf.event_group\",\n","          \"mdf.event_match_number\",\n","          \"mdf.city\",\n","          \"mdf.venue\",\n","          \"mdf.team_type\",\n","          \"mdf.match_type\",\n","          \"mdf.match_result\",\n","          \"mdf.match_won_by_runs\",\n","          \"mdf.match_won_by_wickets\",\n","          \"mdf.match_won_by_innings\",\n","          \"mdf.match_result_method\",\n","          F.concat_ws(\", \", F.from_json(\"umpires\", array_schema)).alias(\"umpires\"), # Concatenate the umpires array into a string, separated by commas\n","          F.coalesce(\"mwn.team_id\", \"elm.team_id\").alias(\"match_winner_id\"), # Get the match winner from Eliminator in case Match Winner is empty (eg. tied matches)\n","          F.col(\"twn.team_id\").alias(\"toss_winner_id\"),\n","          \"mdf.toss_decision\",\n","          F.col(\"pom.player_id\").alias(\"player_of_match_id\"),\n","          F.col(\"fbt.team_id\").alias(\"first_bat_id\"),\n","          F.col(\"flt.team_id\").alias(\"first_field_id\"))\n",")\n","\n","# Create or replace a delta table using the mdf dataframe\n","J.execute_and_log(\n","    function=D.create_or_replace_delta_table,\n","    df=mdf,\n","    lakehouse_name=match_table_path[0],\n","    table_name=match_table_path[1],\n","    schema_name=match_table_path[2],\n","    log_lakehouse=LAKEHOUSE,\n","    job_name=match_table_path[1],\n","    job_category = job_category,\n","    parent_job_name=master_job_name\n","    )"]},{"cell_type":"markdown","id":"4979cfad-56dc-47cc-b4d4-49bacceee304","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["# Create t_dim_date"]},{"cell_type":"code","execution_count":null,"id":"6ed4939b-9080-47ff-b24b-3a2d5d527463","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Read the delta table and assign it an alias\n","mdf = D.read_delta_table(*match_table_path).alias(\"mdf\")\n","\n","# Create a new dataframe with the start and end dates of each year in the match table\n","dte = (\n","    mdf\n","    # Select the minimum and maximum match dates and truncate them to the year level\n","    .select(F.trunc(F.min(\"match_date\"), \"YY\").alias(\"start_date\"),\n","            F.add_months(F.trunc(F.max(\"match_date\"), \"YY\")-1,12).alias(\"end_date\"))\n","    # Generate a sequence of dates from the start to the end date of each year\n","    .select(F.explode(F.sequence(\"start_date\", \"end_date\")).alias(\"date\"))\n","    # Extract the year, quarter, month number and month name from each date\n","    .select(\"date\",\n","            F.year(\"date\").alias(\"year\"),\n","            F.concat(F.lit(\"Q\"), F.quarter(\"date\")).alias(\"quarter\"),\n","            F.month(\"date\").alias(\"month_number\"),\n","            F.date_format(\"date\", \"MMM\").alias(\"month\"))\n",")\n","\n","# Create or replace a delta table with the date dataframe at the given path\n","J.execute_and_log(\n","    function=D.create_or_replace_delta_table,\n","    df=dte,\n","    lakehouse_name=date_table_path[0],\n","    table_name=date_table_path[1],\n","    schema_name=date_table_path[2],\n","    log_lakehouse=LAKEHOUSE,\n","    job_name=date_table_path[1],\n","    job_category = job_category,\n","    parent_job_name=master_job_name\n","    )"]},{"cell_type":"markdown","id":"66ae7369-6ac2-4187-b56f-b13324abcff7","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["# Create t_fact_team_players"]},{"cell_type":"code","execution_count":null,"id":"11a4a049-dcd9-4c91-a415-dcde726c662c","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["tpl = (\n","  tpd\n","  .join(tdf, tdf.team == tpd.team, 'inner' )\n","  .join(pdf, pdf.player_name == tpd.player_name, 'inner' )\n","  .join(mdf, mdf.match_id == tpd.match_id, 'inner')\n","  # Add all relevant foreign keys based on the joined tables\n","  .select(\"tpd.match_id\", \n","          \"tdf.team_id\", \n","          \"pdf.player_id\",\n","          \"mdf.match_date\",\n","          \"mdf.match_winner_id\",\n","          \"mdf.toss_winner_id\",\n","          \"mdf.player_of_match_id\",\n","          \"mdf.first_bat_id\",\n","          \"mdf.first_field_id\")\n",")\n","\n","# Create or replace a Delta table using the tpl DataFrame\n","J.execute_and_log(\n","    function=D.create_or_replace_delta_table,\n","    df=tpl,\n","    lakehouse_name=team_players_table_path[0],\n","    table_name=team_players_table_path[1],\n","    schema_name=team_players_table_path[2],\n","    log_lakehouse=LAKEHOUSE,\n","    job_name=team_players_table_path[1],\n","    job_category = job_category,\n","    parent_job_name=master_job_name\n","    )"]},{"cell_type":"markdown","id":"56c77df3-0042-435e-b9dd-a7cdde5c9eaf","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["# Create t_fact_deliveries"]},{"cell_type":"code","execution_count":null,"id":"baa1ccd8-d907-42de-a504-24d72a68a175","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# Define a list of fields to extract from the JSON data\n","dlv_json_fields = [\n","    [\"batter\", \"string\", \"batter_name\"],\n","    [\"bowler\", \"string\", \"bowler_name\"],\n","    [\"extras.byes\", \"int\", \"byes\"],\n","    [\"extras.legbyes\", \"int\", \"leg_byes\"],\n","    [\"extras.noballs\", \"int\", \"no_balls\"],\n","    [\"extras.wides\", \"int\", \"wides\"],\n","    [\"runs.batter\", \"int\", \"batter_runs\"],\n","    [\"runs.total\", \"int\", \"total_runs\"],\n","    [\"non_striker\", \"string\", \"non_striker_name\"],\n","    [\"wickets[0].kind\", \"string\", \"wicket_kind\"],\n","    [\"wickets[0].player_out\", \"string\", \"player_out\"],\n","    [\"wickets[0].fielders[0].name\", \"string\", \"fielder_name\"]\n","]\n","\n","# Create a list of expressions to select the fields from the JSON data and cast them to the appropriate data types\n","dlv_json_select_lst = [\n","    F.get_json_object(\"col\", \"$.\" + json[0]).cast(json[1]).alias(json[2])\n","    for json in dlv_json_fields\n","]\n","\n","# Read the raw data frame\n","dlv = (\n","  raw_df\n","  .repartition(200) # Repartition the data frame to 200 partitions for better performance\n","  .select(\"match_id\",\n","          F.posexplode(F.from_json(\"match_innings\", array_schema))) # Explode the match_innings array into rows\n","  .select(\"match_id\",\n","          (F.col(\"pos\") + 1).alias(\"innings\"), # Add 1 to the position to get the innings number\n","          F.get_json_object(\"col\",'$.team').alias(\"team\"),  # Get the team name from the JSON object\n","          F.posexplode(F.from_json(F.get_json_object(\"col\",'$.overs'), array_schema))) # Explode the overs array into rows\n","  .select(\"match_id\",\n","          \"innings\",\n","          \"team\",\n","          (F.col(\"pos\") + 1).alias(\"overs\"), # Add 1 to the position to get the over number\n","          F.posexplode(F.from_json(F.get_json_object(\"col\",'$.deliveries'), array_schema))) # Explode the deliveries array into rows\n","  .select(\"*\",\n","          *dlv_json_select_lst) # Select all the columns and the extracted fields from the JSON data\n",")\n","\n","# Create an alias for the data frame\n","dlv = dlv.alias(\"dlv\")\n","\n","# Define a window specification to partition by match_id and order by team_id\n","window_spec = Window.partitionBy(\"match_id\").orderBy(\"team_id\")\n","\n","# Read fact team players\n","tpl = (\n","  D.read_delta_table(*team_players_table_path)\n","  .select(\"match_id\", \"team_id\")\n","  .distinct()\n","  .withColumn(\"next_team_id\", F.coalesce(F.lead(\"team_id\", 1).over(window_spec), F.lag(\"team_id\", 1).over(window_spec))) # Create a new column with the next team_id in the same match using the window function\n","  .alias(\"tpl\")\n",")\n","\n","# Join the data frames to get the batting and bowling team ids, and the player ids for each delivery\n","dlv = (\n","        dlv\n","        .join(mdf, mdf.match_id == dlv.match_id, 'inner')\n","        .join(tdf, tdf.team == dlv.team, 'inner')\n","        .join(tpl, [tpl.team_id == tdf.team_id, tpl.match_id == dlv.match_id], 'inner')\n","        .join(bat, bat.player_name == dlv.batter_name, 'leftouter')\n","        .join(bwl, bwl.player_name == dlv.bowler_name, 'leftouter')\n","        .join(fld, fld.player_name == dlv.fielder_name, 'leftouter')\n","        .join(pot, pot.player_name == dlv.player_out, 'leftouter')\n","        .join(nsr, nsr.player_name == dlv.non_striker_name, 'leftouter')\n","        .select(\"dlv.match_id\",\n","                \"mdf.match_date\",\n","                F.col(\"tdf.team_id\").alias(\"batting_team_id\"),\n","                F.col(\"tpl.next_team_id\").alias(\"bowling_team_id\"),\n","                \"dlv.innings\",\n","                \"dlv.overs\",\n","                (F.col(\"dlv.pos\") + 1).alias(\"balls\"),\n","                F.col(\"bat.player_id\").alias(\"batter_id\"),\n","                F.col(\"nsr.player_id\").alias(\"non_striker_id\"),\n","                F.col(\"bwl.player_id\").alias(\"bowler_id\"),\n","                \"dlv.byes\",\n","                \"dlv.leg_byes\",\n","                \"dlv.no_balls\",\n","                \"dlv.wides\",\n","                \"dlv.batter_runs\",\n","                \"dlv.total_runs\",\n","                \"dlv.wicket_kind\",\n","                F.col(\"pot.player_id\").alias(\"player_out_id\"),\n","                F.col(\"fld.player_id\").alias(\"fielder_id\"))   \n",")\n","\n","# Create or replace the delta table with the deliveries data\n","J.execute_and_log(\n","    function=D.create_or_replace_delta_table,\n","    df=dlv,\n","    lakehouse_name=deliveries_table_path[0],\n","    table_name=deliveries_table_path[1],\n","    schema_name=deliveries_table_path[2],\n","    log_lakehouse=LAKEHOUSE,\n","    job_name=deliveries_table_path[1],\n","    job_category = job_category,\n","    parent_job_name=master_job_name\n","    )\n"]}],"metadata":{"dependencies":{"environment":{"environmentId":"42c57194-1c26-4a76-b1ac-5cefd0660a3e","workspaceId":"8055f488-6591-481e-b384-c01054ed014c"},"lakehouse":{"default_lakehouse":"2b4cedf2-b4c3-4bd8-a763-af94e7196059","default_lakehouse_name":"lh_gold","default_lakehouse_workspace_id":"8055f488-6591-481e-b384-c01054ed014c"}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"synapse_widget":{"state":{},"version":"0.1"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
