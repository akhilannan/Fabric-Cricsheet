{"cells":[{"cell_type":"markdown","source":["# Install Semantic Link"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"b41deb69-dc33-4d24-8b8c-aa488095ceb1"},{"cell_type":"code","source":["!pip install semantic-link --upgrade -q"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"215cc325-49d9-4537-860f-7783fbf9fe76"},{"cell_type":"markdown","source":["# Import Sempy fabric"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"f4c40c49-4098-41be-a721-c294068910b6"},{"cell_type":"code","source":["from sempy import fabric"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7cb9acae-0238-464e-a70f-008751358be0"},{"cell_type":"markdown","source":["# Initialize Variables"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"d4dcac19-cb91-43a5-83b7-9b568bca95d3"},{"cell_type":"code","source":["raw_lakehouse = \"lh_bronze\"\n","clean_lakehouse = \"lh_gold\"\n","workspace_name = \"Cricsheet\" # Target workspace name where the artifacts will be deployed\n","capacity_id = None # None will pick a random capacity which the user has access to\n","start_dataload = True # False if you don't want to start data load"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"tags":["parameters"],"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6777d5f2-9861-4a89-bb42-f985b96c4aa4"},{"cell_type":"markdown","source":["# Get or Create Workspace"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"37237c31-6f39-4afa-a1ce-12d53ffa408d"},{"cell_type":"code","source":["try:\n","    # Attempt to resolve the workspace ID using the provided workspace name\n","    workspace_id = fabric.resolve_workspace_id(workspace_name)\n","except:\n","    # If the workspace ID resolution fails, check if capacity ID is missing\n","    if capacity_id == None:\n","        try:\n","            # List active capacities and select the first one that is not PPU\n","            capacity_id = fabric.list_capacities().query(\"State == 'Active' and Sku != 'PP3'\")[\"Id\"].iloc[0]\n","        except:\n","            # If no suitable capacity is found, exit the notebook\n","            mssparkutils.notebook.exit(\"No Premium/Fabric Capacities found\")\n","    # Create a new workspace using the provided workspace name and capacity ID\n","    workspace_id = fabric.create_workspace(workspace_name, capacity_id)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"98f424d5-f973-4c1a-8699-51f31fd93348"},{"cell_type":"markdown","source":["# Constants"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"82bd5f72-f716-4202-936f-7818893e5a3b"},{"cell_type":"code","source":["REPO_NAME = \"Fabric-Cricsheet\"\n","MAIN_NAME = f\"/{REPO_NAME}-main\"\n","FILES_PATH = '/Files'\n","GIT_PATH = '/git'\n","UTILITY_PATH = '/Environment'\n","NOTEBOOK_PATH = \"/Notebooks\"\n","PACKAGE_FOLDER = 'packages'\n","FUNCTION_FOLDER = 'fabric_python_functions'\n","FABRIC_UTILS_FOLDER = \"fabric_utils\"\n","ENV_NAME = \"cricsheet-environment\"\n","SPARK_CONFIG_FILE = \"Sparkcompute.yml\"\n","FABRIC_UTILS_FILE = f\"{FABRIC_UTILS_FOLDER}.py\"\n","CRICSHEET = {\n","        \"Cricsheet Model\": [\"Cricsheet Analysis\"]\n","    }\n","DATALOAD =  {\n","        \"Data Load Model\": [\"Data Load Monitor\"]\n","    }\n","GITHUB_REPO_ZIP = f\"https://github.com/akhilannan/{REPO_NAME}/archive/refs/heads/main.zip\""],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0dc3c7d3-6f3d-4c8b-8d4c-cfacb1699031"},{"cell_type":"markdown","source":["# Get or create lakehouse details"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"27ba8b0b-d5d5-4c05-a23d-08ff13979f6c"},{"cell_type":"code","source":["def get_or_create_lakehouse(lakehouse_name):\n","    try:\n","        lakehouse_details = mssparkutils.lakehouse.create(name=lakehouse_name, workspaceId = workspace_id)\n","    except:\n","        lakehouse_details = mssparkutils.lakehouse.get(name=lakehouse_name, workspaceId = workspace_id)\n","    lakehouse_id = lakehouse_details.id\n","    abfss_lakehouse_path = f\"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{lakehouse_id}\"\n","    mount_point = f\"/lakehouse/{lakehouse_name}\"\n","    mssparkutils.fs.mount(abfss_lakehouse_path, mount_point)\n","    return mssparkutils.fs.getMountPath(mount_point)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c75834d7-c34c-4229-be4f-77cf1635d4ab"},{"cell_type":"markdown","source":["# Create Mount Points"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"44ec506e-a39d-40ca-8856-5d97950de7d7"},{"cell_type":"code","source":["# Semantic Models and Reports Structure\n","lh_semantic_reports = {\n","    clean_lakehouse: CRICSHEET,\n","    raw_lakehouse: DATALOAD\n","}\n","# Mount lakehouses and get paths\n","lakehouses = {}\n","# Mount lakehouses\n","for lh in lh_semantic_reports:\n","  lakehouses[lh] = get_or_create_lakehouse(lh)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":true},"nteract":{"transient":{"deleting":false}},"collapsed":false,"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"59f31bab-3d0b-4ba3-942f-97f217a1030a"},{"cell_type":"markdown","source":["# Set Variables for Semantic Model and Report"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"69b3fce1-2389-4b3f-a929-e2b0db3641b4"},{"cell_type":"code","source":["semantic_model_report = []\n","lh_files_path = lakehouses[raw_lakehouse] + FILES_PATH\n","lh_git_path = lh_files_path + GIT_PATH \n","lh_repo_path = lh_git_path + MAIN_NAME\n","\n","for lakehouse, models in lh_semantic_reports.items():\n","    for model, reports in models.items():\n","        bim_path = f\"/Semantic Model/{model}.SemanticModel\"\n","        for report in reports:\n","            report_path = f\"/Power BI Report/{report}.Report\"\n","            semantic_model_report.append({\n","                'lakehouse_name': lakehouse,\n","                'semantic_model': model,\n","                'semantic_model_path': lh_repo_path + bim_path,\n","                'report_path': lh_repo_path + report_path\n","            })"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fed783c3-23a2-47f8-8305-5f1eef8d68a1"},{"cell_type":"markdown","source":["# Set Additional Variables"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"c6ca697f-e814-4339-8b22-6f19ee6cf96d"},{"cell_type":"code","source":["import os\n","lh_git_notebook_path = lh_repo_path + NOTEBOOK_PATH\n","git_utils_path = lh_repo_path + UTILITY_PATH + '/' + FABRIC_UTILS_FILE\n","spark_config_path = lh_repo_path + UTILITY_PATH + '/' + SPARK_CONFIG_FILE\n","package_path = os.path.join(lh_files_path, PACKAGE_FOLDER)\n","function_folder_path = os.path.join(package_path, FUNCTION_FOLDER)\n","fabric_utils_path = os.path.join(function_folder_path, FABRIC_UTILS_FOLDER)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"75b41dc2-2297-4592-ae65-0ae7e0df75d9"},{"cell_type":"markdown","source":["# Download Git Repo contents as Zip and Unzip it"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"02df99c7-fb13-4833-b0dd-69091d97fd32"},{"cell_type":"code","source":["import requests\n","import zipfile\n","import io\n","import shutil\n","\n","shutil.rmtree(lh_git_path, ignore_errors=True)\n","r = requests.get(GITHUB_REPO_ZIP)\n","z = zipfile.ZipFile(io.BytesIO(r.content))\n","z.extractall(lh_git_path)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"79dc300b-eebe-4438-a96b-9962f37949af"},{"cell_type":"markdown","source":["# Add and Import Fabric Utilities"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"07dc64cd-95a6-4f35-a07e-9740cc508386"},{"cell_type":"code","source":["sc.addPyFile(git_utils_path)\n","import fabric_utils as U"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d9eddcf7-d194-498d-b8ed-ebfdecc2a834"},{"cell_type":"markdown","source":["# Create and Publish Spark Environment"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9c4b855e-707a-4fb1-aa01-f1b07a489d3b"},{"cell_type":"code","source":["U.create_and_publish_spark_environment(ENV_NAME, spark_config_path, git_utils_path, workspace_id)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c1299cba-4d21-4ad1-b33e-8c6dc70a8814"},{"cell_type":"markdown","source":["# Deploy Notebooks"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"492d69c3-5e35-4e28-afe8-99e68de08b49"},{"cell_type":"code","source":["import json\n","\n","# Define the code replacements using formatted strings\n","code_replacements = {\n","    'RAW_LAKEHOUSE = \"lh_bronze\"': f'RAW_LAKEHOUSE = \"{raw_lakehouse}\"',\n","    'CLEAN_LAKEHOUSE = \"lh_gold\"': f'CLEAN_LAKEHOUSE = \"{clean_lakehouse}\"'\n","}\n","\n","# Define the mapping of notebook names to their respective default lakehouse\n","notebook_lakehouse = {\n","    'Cricsheet Orchestrator': raw_lakehouse,\n","    'Cricsheet Initialize': raw_lakehouse,\n","    'Cricsheet Ingest Data': raw_lakehouse,\n","    'Cricsheet Build Facts and Dimensions': clean_lakehouse,\n","    'Cricsheet Model Refresh': clean_lakehouse,\n","    'Cricsheet Optimize and Vacuum': raw_lakehouse\n","}\n","\n","# Iterate over the notebook_lakehouse dictionary and process each notebook\n","for file_name, lakehouse_type in notebook_lakehouse.items():\n","    full_file_name = file_name + \".ipynb\"\n","    file_path = os.path.join(lh_git_notebook_path , full_file_name)\n","    with open(file_path, 'r') as file:\n","        notebook_json = json.load(file)\n","    U.create_or_replace_notebook_from_ipynb(file_name, notebook_json, lakehouse_type, ENV_NAME, code_replacements, workspace_id)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7935622a-9e10-483f-9970-64fd83a96dc9"},{"cell_type":"markdown","source":["# Deploy Semantic Models and Reports"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"3a5c16bd-67dc-4ec8-b14f-d36ed2883e81"},{"cell_type":"code","source":["# Define a function to process each semantic model report\n","def deploy_model_and_report(smr):\n","\n","    # Create or replace the semantic model from BIM\n","    U.execute_with_retries(U.create_or_replace_semantic_model,\n","                           model_path=smr['semantic_model_path'],\n","                           workspace_id = workspace_id)\n","    # Update the model expression\n","    U.execute_with_retries(U.update_model_expression,\n","                           dataset_name=smr['semantic_model'],\n","                           lakehouse_name=smr['lakehouse_name'],\n","                           workspace_id = workspace_id)\n","    # Create or replace the report from report JSON\n","    U.execute_with_retries(U.create_or_replace_report_from_pbir,\n","                           report_path=smr['report_path'],\n","                           dataset_name=smr['semantic_model'],\n","                           dataset_workspace_id = workspace_id,\n","                           report_workspace_id = workspace_id)\n","\n","# Iterate over each semantic model report and process it\n","for smr in semantic_model_report:\n","    deploy_model_and_report(smr)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"591429c9-d061-488c-9890-0e592cfd28d5"},{"cell_type":"markdown","source":["# Delete Git folder"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"4fc5bd16-bfc0-411a-b1d6-04a1bc20f4bb"},{"cell_type":"code","source":["shutil.rmtree(lh_git_path, ignore_errors=True)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d6d874f0-8556-4ac6-9ab3-03eedc12b07a"},{"cell_type":"markdown","source":["# Start Data Load"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"dcbff0b4-d253-42cc-88cb-210d22b61604"},{"cell_type":"code","source":["if start_dataload:\n","    cricsheet_orchestrator_id = U.get_item_id('Cricsheet Orchestrator', 'Notebook', workspace_id)\n","    try:\n","        fabric.run_notebook_job(notebook_id = cricsheet_orchestrator_id, workspace = workspace_id)\n","    except Exception as e:\n","        print(f\"{e}. Check the run details from Monitoring Hub or by opening the Data Load Monitor report.\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a49aaaec-57a1-44c2-b5ff-4784b000bbc2"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"},"language_group":"synapse_pyspark"},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}