{"cells":[{"cell_type":"markdown","source":["# Initialize Lakehouse Names"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"d4dcac19-cb91-43a5-83b7-9b568bca95d3"},{"cell_type":"code","source":["raw_lakehouse = \"lh_bronze\"\n","clean_lakehouse = \"lh_gold\""],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"tags":["parameters"]},"id":"6777d5f2-9861-4a89-bb42-f985b96c4aa4"},{"cell_type":"markdown","source":["# Set Git URLs"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"ad1b9506-fe8d-43c0-b766-d64596e90fea"},{"cell_type":"code","source":["GITHUB_LINK = \"https://github.com/akhilannan/Fabric-Cricsheet\"\n","RAW_PATH = \"/raw/main\"\n","SEMANTIC_MODEL = \"Cricsheet Model\"\n","REPORT_NAME = \"Cricsheet Analysis\"\n","BASE_PATH = GITHUB_LINK + RAW_PATH\n","BIM_URL = f\"{BASE_PATH}/Semantic Model/{SEMANTIC_MODEL}.bim\"\n","REPORT_URL = f\"{BASE_PATH}/Power%20BI%20Report/{REPORT_NAME}.Report/report.json\"\n","NOTEBOOK_URL = f\"{BASE_PATH}/Notebooks\"\n","UTILS_URL = f\"{NOTEBOOK_URL}/Fabric%20Utilities.ipynb\""],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"890b7ad1-ade9-4059-8ddc-da7800a96af5"},{"cell_type":"markdown","source":["# Initialize Common Functions and Libraries"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"ffc37a18-0a8b-4c20-8265-ca59db895e95"},{"cell_type":"code","source":["import requests\n","\n","def download_notebook_code(url):\n","    # Get the JSON representation of the notebook\n","    ipynb_json = requests.get(url).json()\n","    \n","    # Initialize variables to store the Python code and function names\n","    python_code = \"\"\n","    function_names = []\n","\n","    # Iterate through the cells in the notebook\n","    for cell in ipynb_json['cells']:\n","        # Check if the cell is a code cell\n","        if cell['cell_type'] == 'code':\n","            # Extract the source code from the cell\n","            source_code = ''.join([\n","                f\"get_ipython().system('{line.strip()[1:]}')\" if line.strip().startswith('!pip install')\n","                else line for line in cell['source']\n","            ])\n","            \n","            # Add the source code to the Python code string\n","            python_code += source_code + '\\n\\n'\n","            \n","            # Find function definitions and add their names to the list\n","            function_names += [line.split()[1].split('(')[0] for line in cell['source'] if line.strip().startswith('def ')]\n","\n","    # Return the Python code and the list of function names\n","    return python_code, function_names\n","\n","python_code, function_names = download_notebook_code(UTILS_URL)\n","exec(python_code)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"0c22a44e-9505-446d-abba-28091b7db165"},{"cell_type":"markdown","source":["# Create Wheel Package"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"a99f1e45-4781-4b10-a073-00a640cd81eb"},{"cell_type":"code","source":["import os\n","import subprocess\n","import sys\n","import shutil\n","\n","def create_python_package(lakehouse, python_code, function_names):\n","    # Define the path to the directory where the Python file will be created\n","    package_path = os.path.join(get_lakehouse_path(lakehouse, \"local\", \"Files\"), \"packages\")\n","    fabric_path = os.path.join(package_path, \"fabric_python_functions\")\n","    fabric_utils_path = os.path.join(fabric_path, \"fabric_utils\")\n","    \n","    # Remove the existing package directory and create a new one\n","    shutil.rmtree(package_path, ignore_errors=True)\n","    os.makedirs(fabric_utils_path, exist_ok=True)\n","\n","    # Write the Python code to the functions.py file\n","    python_file_path = os.path.join(fabric_utils_path, 'functions.py')\n","    with open(python_file_path, 'w') as python_file:\n","        python_file.write(python_code)\n","\n","    # Write the __init__.py content\n","    init_content = \"\\n\".join(f\"from .functions import {name}\" for name in function_names)\n","    init_file_path = os.path.join(fabric_utils_path, '__init__.py')\n","    with open(init_file_path, 'w') as init_file:\n","        init_file.write(init_content)\n","\n","    # Write the setup.py content\n","    setup_content = \"\"\"\n","from setuptools import setup, find_packages\n","\n","setup(\n","    name='fabric_utils',\n","    version='0.1',\n","    packages=find_packages(),\n","    install_requires=['semantic-link'],\n",")\n","\"\"\"\n","    setup_file_path = os.path.join(fabric_path, 'setup.py')\n","    with open(setup_file_path, 'w') as setup_file:\n","        setup_file.write(setup_content)\n","\n","    # Build the wheel file\n","    os.chdir(fabric_path)\n","    subprocess.check_call([sys.executable, 'setup.py', 'sdist', 'bdist_wheel'])\n","\n","    # Move the wheel file outside the 'fabric_utils' folder\n","    wheel_files = [f for f in os.listdir('dist') if f.endswith('.whl')]\n","    for file in wheel_files:\n","        shutil.move(f'dist/{file}', f'../{file}')\n","\n","    # Clean up the package directory\n","    shutil.rmtree(fabric_path)\n","\n","    # Print a success message\n","    print(\"The wheel file has been successfully created and moved outside the 'fabric_utils' folder.\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"6abde91b-7712-45bb-81c8-e9f6e745f171"},{"cell_type":"markdown","source":["# Create Lakehouse and Push wheel file to Files folder in Lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"32820785-ccaf-4f5e-8744-6f84c530f7fd"},{"cell_type":"code","source":["for lakehouse in [raw_lakehouse, clean_lakehouse]:\n","    create_lakehouse_if_not_exists(lakehouse)\n","    create_python_package(lakehouse, python_code, function_names)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":true},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"0633846c-8da2-458a-babf-a0dbcfd56567"},{"cell_type":"markdown","source":["# Deploy Notebook"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"492d69c3-5e35-4e28-afe8-99e68de08b49"},{"cell_type":"code","source":["# Use a function to handle the notebook processing\n","def process_notebook(file_name, default_lakehouse, notebook_url, code_replacements):\n","    file_name_w_extn = f\"{file_name}.ipynb\"\n","    link = f\"{notebook_url}/{file_name_w_extn}\"\n","    notebook_json = download_and_update_notebook(link, code_replacements)\n","    create_or_replace_notebook_from_ipynb(file_name, notebook_json, default_lakehouse)\n","\n","# Define the code replacements using formatted strings\n","code_replacements = {\n","    'RAW_LAKEHOUSE = \"lh_bronze\"': f'RAW_LAKEHOUSE = \"{raw_lakehouse}\"',\n","    'CLEAN_LAKEHOUSE = \"lh_gold\"': f'CLEAN_LAKEHOUSE = \"{clean_lakehouse}\"'\n","}\n","\n","# Define the mapping of notebook names to their respective default lakehouse\n","notebook_lakehouse = {\n","    'Cricsheet Orchestrator': raw_lakehouse,\n","    'Cricsheet Initialize': raw_lakehouse,\n","    'Cricsheet Ingest Data': raw_lakehouse,\n","    'Cricsheet Build Facts and Dimensions': clean_lakehouse,\n","    'Cricsheet Model Refresh': clean_lakehouse,\n","    'Cricsheet Optimize and Vacuum': raw_lakehouse\n","}\n","\n","# Iterate over the notebook_lakehouse dictionary and process each notebook\n","for file_name, lakehouse_type in notebook_lakehouse.items():\n","    process_notebook(file_name, lakehouse_type, NOTEBOOK_URL, code_replacements)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"7935622a-9e10-483f-9970-64fd83a96dc9"},{"cell_type":"markdown","source":["# Deploy Semantic Model"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"9f6ded29-5c75-4348-9632-850aa83d6bbc"},{"cell_type":"code","source":["bim_json = requests.get(BIM_URL).json()\n","execute_with_retries(create_or_replace_semantic_model_from_bim,\n","                     dataset_name=SEMANTIC_MODEL,\n","                     bim_file_json=bim_json)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"4b967ee7-e62a-4959-948d-79b20436f3c3"},{"cell_type":"markdown","source":["# Update Semantic Model Connection"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"bd82ce92-c688-4239-9b21-7ef1199ec4ea"},{"cell_type":"code","source":["execute_with_retries(update_model_expression,\n","                     dataset_name=SEMANTIC_MODEL,\n","                     lakehouse_name=clean_lakehouse)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"8caaf622-d0a3-4cfe-8737-1e3db0b0e12d"},{"cell_type":"markdown","source":["# Deploy Report and Repoint to the Semantic Model"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"f1f6695a-a533-4a6b-9413-fbfffb33c58e"},{"cell_type":"code","source":["report_json = requests.get(REPORT_URL).json()\n","execute_with_retries(create_or_replace_report_from_reportjson,\n","                     report_name=REPORT_NAME,\n","                     dataset_name=SEMANTIC_MODEL,\n","                     report_json=report_json)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"b73e8772-6e3f-42b8-8ebd-7b74b5862139"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python"},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}