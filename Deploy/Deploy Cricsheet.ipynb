{"cells":[{"cell_type":"markdown","source":["# Initialize Variables"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"d4dcac19-cb91-43a5-83b7-9b568bca95d3"},{"cell_type":"code","source":["raw_lakehouse = \"lh_bronze\"\n","clean_lakehouse = \"lh_gold\"\n","workspace_name = \"Cricsheet\" # Target workspace name where the artifacts will be deployed\n","capacity_id = None # None will pick a random capacity which the user has access to\n","start_dataload = True # False if you don't want to start data load\n","wait_for_dataload_completion = True # False if you don't want to wait for the completion of job\n","# Below variables required only when running outside Fabric\n","tenant_id = None\n","client_id = None\n","client_secret = None"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"tags":["parameters"],"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6777d5f2-9861-4a89-bb42-f985b96c4aa4"},{"cell_type":"markdown","source":["# Constants"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"82bd5f72-f716-4202-936f-7818893e5a3b"},{"cell_type":"code","source":["REPO_NAME = \"Fabric-Cricsheet\"\n","BRANCH = \"main\"\n","REPO_BRANCH_NAME = f\"/{REPO_NAME}-{BRANCH}\"\n","ROOT_DOWNLOAD_FOLDER = 'git'\n","ENV_PATH = '/Environment'\n","NOTEBOOK_PATH = \"/Notebooks\"\n","ENV_NAME = \"cricsheet-environment\"\n","SPARK_CONFIG_FILE = \"Sparkcompute.yml\"\n","UTILS = [\"api_client\", \"fabric_utils\", \"delta_table_operations\", \"file_operations\", \"job_operations\", \"powerbi_operations\", \"environment_operations\"]\n","UTILS_PY = [util + \".py\" for util in UTILS]\n","CRICSHEET = {\n","        \"Cricsheet Model\": [\"Cricsheet Analysis\"]\n","    }\n","DATALOAD =  {\n","        \"Data Load Model\": [\"Data Load Monitor\"]\n","    }\n","GITHUB_REPO = f\"https://github.com/akhilannan/{REPO_NAME}\"\n","GITHUB_RAW = f\"/raw/{BRANCH}\"\n","ZIP_FILE_NAME = f\"{BRANCH}.zip\"\n","GITHUB_REPO_ZIP = f\"{GITHUB_REPO}/archive/refs/heads/{ZIP_FILE_NAME}\""],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0dc3c7d3-6f3d-4c8b-8d4c-cfacb1699031"},{"cell_type":"markdown","source":["# Load Common Functions from Git"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f4e507ac-be29-416c-9670-5cec59aa6950"},{"cell_type":"code","source":["git_url_files = [GITHUB_REPO + GITHUB_RAW + ENV_PATH + '/' + util for util in UTILS_PY]\n","for git_url_file in git_url_files:\n","    sc.addPyFile(git_url_file)\n","from api_client import FabricPowerBIClient\n","import fabric_utils as U\n","import file_operations as L\n","import job_operations as J\n","import powerbi_operations as P\n","import environment_operations as E"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8776ca9e-6ab6-4f11-a4ad-bfdeb93c9bbe"},{"cell_type":"markdown","source":["# Initialize Fabric Client"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d06eccf5-be37-4955-b2bc-60478979eed5"},{"cell_type":"code","source":["fabric_client = FabricPowerBIClient(tenant_id = tenant_id, client_id = client_id, client_secret = client_secret, client_type = 'fabric')"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"db5ad7f0-d771-460b-a2e1-c682bb318512"},{"cell_type":"markdown","source":["# Create Workspace if not exists"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"33c58871-84f3-48b1-8ba2-c80a52e44910"},{"cell_type":"code","source":["workspace_id = U.get_or_create_fabric_workspace(workspace_name, capacity_id, client=fabric_client)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9107d1d3-33b2-41b8-a46b-e7715fcd1192"},{"cell_type":"markdown","source":["# Set Lakehouse Path"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"077c8b29-448b-4b3d-9597-4d1bdd3d146a"},{"cell_type":"code","source":["# Semantic Models and Reports Structure\n","lh_semantic_reports = {\n","    clean_lakehouse: CRICSHEET,\n","    raw_lakehouse: DATALOAD\n","}\n","# Mount lakehouses and get paths\n","lakehouses = {}\n","# Mount lakehouses\n","for lh in lh_semantic_reports:\n","  lakehouses[lh] = U.get_lakehouse_path(lakehouse_name = lh, path_type = \"local\", folder_type = \"Files\", workspace = workspace_id, client=fabric_client)\n","\n","lh_repo_path = lakehouses[raw_lakehouse] + \"/\" + ROOT_DOWNLOAD_FOLDER + REPO_BRANCH_NAME"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":true},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"957c6a9d-2c49-464b-9a02-fe4da83523bf"},{"cell_type":"markdown","source":["# Download Git Repo contents as Zip and Unzip it"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"02df99c7-fb13-4833-b0dd-69091d97fd32"},{"cell_type":"code","source":["zip_file_path = L.download_data(GITHUB_REPO_ZIP, raw_lakehouse, ROOT_DOWNLOAD_FOLDER, workspace_id, client=fabric_client)\n","L.unzip_files(zip_file_path)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":true},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"efdb99ab-8595-4584-ae9e-1bf138ce0a73"},{"cell_type":"markdown","source":["# Create and Publish Spark Environment"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9c4b855e-707a-4fb1-aa01-f1b07a489d3b"},{"cell_type":"code","source":["lh_git_env_path = lh_repo_path + ENV_PATH\n","spark_config_path = lh_git_env_path + '/' + SPARK_CONFIG_FILE\n","utils_files = [lh_git_env_path + '/' + util for util in UTILS_PY]\n","E.create_and_publish_spark_environment(ENV_NAME, spark_config_path, utils_files, workspace_id, client=fabric_client)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c1299cba-4d21-4ad1-b33e-8c6dc70a8814"},{"cell_type":"markdown","source":["# Deploy Notebooks"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"492d69c3-5e35-4e28-afe8-99e68de08b49"},{"cell_type":"code","source":["# Define the code replacements using formatted strings\n","code_replacements = {\n","    'RAW_LAKEHOUSE = \"lh_bronze\"': f'RAW_LAKEHOUSE = \"{raw_lakehouse}\"',\n","    'CLEAN_LAKEHOUSE = \"lh_gold\"': f'CLEAN_LAKEHOUSE = \"{clean_lakehouse}\"'\n","}\n","\n","# Define the mapping of notebook names to their respective default lakehouse\n","notebook_lakehouse = {\n","    'Cricsheet Orchestrator': raw_lakehouse,\n","    'Cricsheet Initialize': raw_lakehouse,\n","    'Cricsheet Ingest Data': raw_lakehouse,\n","    'Cricsheet Build Facts and Dimensions': clean_lakehouse,\n","    'Cricsheet Model Refresh': clean_lakehouse,\n","    'Cricsheet Optimize and Vacuum': raw_lakehouse\n","}\n","\n","# Iterate over the notebook_lakehouse dictionary and process each notebook\n","lh_git_notebook_path = lh_repo_path + NOTEBOOK_PATH\n","for file_name, lakehouse_type in notebook_lakehouse.items():\n","    file_path = lh_git_notebook_path + \"/\" + file_name + \".ipynb\"\n","    E.create_or_replace_notebook_from_ipynb(file_path, lakehouse_type, ENV_NAME, code_replacements, workspace_id, client=fabric_client)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7935622a-9e10-483f-9970-64fd83a96dc9"},{"cell_type":"markdown","source":["# Deploy Semantic Models and Reports"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"3a5c16bd-67dc-4ec8-b14f-d36ed2883e81"},{"cell_type":"code","source":["# Define a function to process each semantic model report\n","def deploy_model_and_report(smr, workspace_id):\n","    # Create or replace the semantic model from BIM\n","    J.execute_with_retries(P.create_or_replace_semantic_model,\n","                           model_path=smr['semantic_model_path'],\n","                           lakehouse_name=smr['lakehouse_name'],\n","                           workspace=workspace_id, \n","                           client=fabric_client)\n","    # Create or replace the report from report JSON\n","    J.execute_with_retries(P.create_or_replace_report_from_pbir,\n","                           report_path=smr['report_path'],\n","                           dataset_name=smr['semantic_model'],\n","                           dataset_workspace=workspace_id,\n","                           report_workspace=workspace_id, \n","                           client=fabric_client)\n","\n","# Set Variables for Semantic Model and Report\n","semantic_model_report = [\n","    {\n","        'lakehouse_name': lakehouse,\n","        'semantic_model': model,\n","        'semantic_model_path': f\"{lh_repo_path}/Semantic Model/{model}.SemanticModel\",\n","        'report_path': f\"{lh_repo_path}/Power BI Report/{report}.Report\"\n","    }\n","    for lakehouse, models in lh_semantic_reports.items()\n","    for model, reports in models.items()\n","    for report in reports\n","]\n","\n","# Iterate over each semantic model report and process it\n","for smr in semantic_model_report:\n","    deploy_model_and_report(smr, workspace_id)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"80a16167-9e76-485b-a37e-bc451ae3492b"},{"cell_type":"markdown","source":["# Delete Git folder"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"4fc5bd16-bfc0-411a-b1d6-04a1bc20f4bb"},{"cell_type":"code","source":["L.delete_folder_from_lakehouse(raw_lakehouse, ROOT_DOWNLOAD_FOLDER, workspace_id, client=fabric_client)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":true},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"d6d874f0-8556-4ac6-9ab3-03eedc12b07a"},{"cell_type":"markdown","source":["# Start Data Load"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"dcbff0b4-d253-42cc-88cb-210d22b61604"},{"cell_type":"code","source":["if start_dataload:\n","    J.run_notebook_job('Cricsheet Orchestrator', wait_for_dataload_completion, workspace_id, client=fabric_client)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a49aaaec-57a1-44c2-b5ff-4784b000bbc2"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"},"language_group":"synapse_pyspark"},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}